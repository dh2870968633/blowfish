
[{"content":" 第一阶段面试题 # Linux # 说 10 个常用的 Linux 命令 # 重启网络：systemctl restart network\n防火墙相关命令：\n查看防火墙状态：systemctl status firewalld 本次服务内关闭防火墙（重启虚拟机后会自动开启）：systemctl stop firewalld 启动防火墙：systemctl start firewalld 重启防火墙：systemctl restart firewalld 禁用防火墙服务：systemctl disable firewalld 打开编辑器：\nvi 文件path vim 文件path :wq 保存退出 :q 直接退出 :q!强制退出 :set nu 显示行号 O切换到上一行 o切换到下一行 关闭虚拟机命令：\nhalt ：相当于直接拔掉电源，不推荐。 poweroff ：直接关闭机器，但是有可能当前虚拟机有其他用户正在使用，不推荐。 shutdown -h now ：马上关闭计算机 ，但是会给其他用户发送消息，推荐。 reboot ：重启虚拟机。 cd 改变当前工作目录\ncd - 返回到上一次所在目录 cd -P 如果切换的目标目录是一个符号链接，则直接切换到符号链接指向的目标目录（不走软链接，走真实地址） cd -L 如果切换的目标目录是一个符号链接，则直接切换到符号链接名所在的目录（走软链接，cd /xxx 默认就是 -L） 列出指定目录下的所有文件\nls 只显示文件名 ll 显示详细的文件信息（文件名、权限、用户、组、创建时间……） 文件类型分类 普通文件 d 文件夹 l 软连接 创建文件目录 mkdir\nmkdir -p a/b/c/d/e/f 创建多级目录 拷贝文件或者文件目录 cp\ncp 源文件 目标目录 拷贝文件夹：cp -r lucky /opt mv\n移动文件或者文件夹：mv abc /opt 修改文件名称：mv a abcd rm\n删除文件：rm 文件 rm -f 文件 -f 表示不询问 yes 或or 删除文件夹：rm -f 文件夹 rm -rf 文件夹 -r递归删除包括子目录 创建文件 touch\n如果没有就创建一个文件 如果该文件已经存在，修改文件的三个时间，将三个时间改为当前时间 查看文件状态 stat\n读取文件信息\ncat 监听文件内容 默认屏幕输出显示 tac 按行逆序显示文件内容 tail 从文件末尾读取n行 tail profile -n tail -f 监听文件更新的内容 文件删除再创建就监听不了了 tail -F 监听文件更新的内容 文件删除再创建会重新监听 grep \u0026ldquo;字符\u0026rdquo; 字符匹配 find 查找指定文件 计算机之间的数据传输\nWindow - Linux 需要手动安装：yum -y install lrzsz rz：将文件从window上传到Linux sz 文件：将文件从Linux传输到Window Linux - Linux scp 源数据地址(source) 目标数据地址(target) scp root@192.168.88.100:/etc/profile / scp /etc/profile root@192.168.88.100: / rsync 源数据地址(source) 目标数据地址(target) 文件压缩\ntar\n解压缩 tar -zx(解压)v(过程)f(文件) lucky.tar.gz\n压缩\ntar -zc(压缩)f(文件) tomcat.tar.gz(压缩后的名字) apache-tomcat-7.0.61(源文件)\ntar -zxf tomcat.tar.gz -C /opt/ -C 指定解压缩的文件目录\nzip和unzip\nyum install zip unzip -y 压缩 zip -r tomcat.zip apache-tomcat-7.0.61 解压缩 unzip tomcat.zip 网络相关命令\n查看当前网卡的配置信息 ifconfig 如果没有ifconfig ，可以使用ip addr 临时代替 查看当前网络的状态信息 netstat 查看与目标IP地址是否能够连通 ping 时间命令\n查看当前系统时间 date 查看日历 cal 修改时间 date -s 格式 用户-组-权限\n用户 useradd [选项] [用户名] 添加用户 passwd 用户名 设置用户密码 id 用户名 查看用户 删除用户（默认会删除组和家目录） userdel -r 用户名 切换用户 su - 用户名 修改权限 修改文件所属用户和组信息命令为： chown 用户:组 文件名 chmod 644 file2.txt 修改文件的权限 管道，重定向，| \u0026raquo; \u0026gt; \u0026lt;\nLinux 系统中创建用户，用户组的命令 # 创建用户： 创建用户：useradd [选项] [用户名] 创建用户并指定组、附属组：useradd -g [用户名] [组名] -G [组名],[组名] -g 指定用户对应的用户组 -G 指定用户对应的附属组 设置密码：passwd [用户名] 查看用户：id [用户名] 删除用户： userdel -r [用户名] 创建用户组： 创建组：groupadd [组名] 修改组：groupmod -n newgroup oldgroup 删除组：groupdel [组名] 查看用户对应的组：groups [组名] 修改用户的组： usermod [用户名] -g [组名] -g 修改主组（只能一个） usermod [用户名] -G [组名1] ,[组名2] -G 修改附属组（可以多个） Linux 修改文件所属的命令，修改文件权限的命令 # 修改文件所属： chown 用户:组 文件名 chown zhangsan:bigdata file1.txt chown -R zhangsan:bigdata t1/ 修改文件夹时，-R 让子目录迭代修改 修改文件权限： UGO 模型 ugo 分别表示User 用户、Group 组用户、Other 其他用户 chmod u+x file2.txt chmod go+wx file2.txt chmod ugo-wx file2.txt 777模型： chmod 644 file2.txt 644=rw-r\u0026ndash;r\u0026ndash; 将 rwx 看做二进制数，有权限表示 1，没有表示 0。那么 rwx 就可以表示为：111，二进制 111 就是十进制 7。 用户目录在哪，环境变量有几种配置方式 # 用户的目录在哪？ 用户的主目录（也称为家目录）通常位于/home目录下，每个用户的主目录都是以该用户的用户名命名的子目录。例如，如果系统中有一个用户名为john的用户，他的主目录将位于/home/john。 配置环境变量的几种方式？ export 命令设置 export PATH=${PATH}:${TOMCAT_HOME}/bin 只对当前会话有效\u0026ndash;退出当前终端失效\u0026ndash;临时的 .bashrc 或 .bash_profile文件设置 系统级的位于 /etc/bashrc ,对所有用户生效 用户级的 ~/.bashrc ，对当前用户生效。 设置后 source命令让文件立即生效 /etc/profile文件设置 系统级的环境变量 对所有用户生效 但要有权限 设置后 source命令让文件立即生效 在 /etc/profile.d 目录下新增 .sh 格式的自定义环境变量文件 /etc/profile文件会对自定义的环境变量文件进行遍历把环境变量加载进来，并执行生效 Linux 安装软件的方式有几种，分别什么区别 # 使用包管理器对安装包进行安装 rpm [选项] 软件包 自己去官网下载好所需软件的rpm包上传至服务器。 或直接使用wget 命令下载至服务器。 tar [选项] 压缩包 以 Tomcat 为例，准备好压缩包 apache-tomcat-9.0.72.tar.gz ，可以去官网自己下载然后上传至服务器，或直接使用 wget 命令下载至服务器。 tar -zxf apache-tomcat-9.0.72.tar.gz -C /opt/yjx/ yum安装 yum 命令是基于 RPM 的软件包管理器，它可以使系统管理人员交互和自动化地更新与管理 RPM 软件包，能够从指定的服务器自动下载 RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 源码安装 去官网下载所需软件的源码包上传至服务器，或者直接使用 wget 下载至服务器。 安装之前要先安装它所依赖的环境。 创建安装目录mkdir -p /opt/yjx/nginx 配置安装目录./configure \u0026ndash;prefix=/opt/yjx/nginx/ 解压源码包后 编译并安装 make \u0026amp;\u0026amp; make install 如何选择 Linux 操作系统版本 # 选择 Linux 操作系统版本时，需要考虑多个因素，以确保所选版本满足你的具体需求和环境。以下是一些关键点，可以帮助你做出更明智的选择：\n用途：\n桌面环境：如果你打算在 Linux 上工作或娱乐，可能会倾向于选择一个用户界面友好、软件丰富、驱动支持良好的发行版，如 Ubuntu、Mint 或 Fedora。 服务器环境：服务器通常需要稳定、安全、长期支持的发行版，如 CentOS、RHEL（Red Hat Enterprise Linux）、Debian 或 Ubuntu Server。 开发环境：开发人员可能更关心最新的编程工具和库，以及快速的更新周期，Fedora 或 Arch Linux 可能满足这类需求。 技术支持和社区：\n选择有活跃社区和良好文档支持的发行版，可以获得更好的问题解答和学习资源。 商业支持也是一个重要因素，尤其是对于企业用户，RHEL 和 SUSE Linux Enterprise 提供专业的技术支持。 稳定性与更新周期：\n稳定性：一些发行版，如 Debian Stable 和 RHEL，注重长期稳定性和安全性，更新周期较长，适合企业级服务器。 更新周期：其他发行版，如 Fedora 和 Arch Linux，更新频繁，可以及时获得最新的软件和安全补丁，但可能不够稳定。 安全性：\n考虑发行版的安全特性，如 SELinux（安全增强型 Linux）和 AppArmor，它们可以提供额外的安全防护。 硬件兼容性：\n确保所选发行版与你的硬件兼容，包括处理器架构、图形卡驱动等。例如，ARM 架构的设备可能需要特定的 ARM 构建版本。 定制和灵活性：\n一些发行版，如 Arch Linux 和 Gentoo，提供了高度的定制性，适合高级用户和系统管理员。 资源消耗：\n考虑你的设备的资源（如 CPU、内存和存储），轻量级发行版，如 Puppy Linux 或 Lubuntu，适合老旧或资源有限的硬件。 软件生态：\n检查发行版的软件仓库，确保你能找到所需的软件包。流行的发行版通常拥有丰富的软件库。 桌面环境：\n如果是桌面用户，考虑你喜欢的桌面环境（如 GNOME、KDE、XFCE 或 MATE），并选择支持该环境的发行版。 个人偏好和经验：\n最终选择可能还会受到个人喜好和以往使用经验的影响。 总之，选择 Linux 发行版是一个综合考量的过程，没有绝对的好坏之分，只有更适合你的那个版本。建议尝试几个不同的发行版，看看哪一个最符合你的需求和偏好。对于初学者，通常推荐从用户友好且文档丰富的发行版开始，如 Ubuntu 或 Mint。对于有经验的用户或特定目的，可以根据上述指导原则进行更深入的比较和选择。\nLinux 服务器之间免密是如何实现的 # 生成密钥对：\n在源服务器上执行ssh-keygen -t rsa -P \u0026rsquo;\u0026rsquo; -f ~/.ssh/id_rsa命令，生成一个 RSA 密钥对，公钥将被保存在 ~/.ssh/id_rsa.pub 文件中，而私钥则保存在 ~/.ssh/id_rsa 文件中。 拷贝公钥\n想免秘钥登录谁，只需要把自己的公钥传递给对方主机即可，这个公钥文件必须放在对方主机的 ~/.ssh/authorized_keys 文件中。可以使用命令将公钥文件自动传递过去，ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.88.102 原理：采用的非对称算法\n公钥加密 私钥解密 node01把公钥给了node02，node01登录node02时，先检查ak文件中是否有node01的公钥，如果有，就随机生成一个字符串，用该公钥加密，将加密后的密文发给node01，node01接收该密文，用私钥进行解密，将解密后的明文再发给node02，node02拿该明文与最初生成的字符串比较，比对成功就免密！ Shell 脚本第一行是什么，运行 Shell 脚本的方式有哪些，有什么区别 # **Shell脚本的第一行通常被称为shebang（井号感叹号）行，这是因为这一行以 #! 开头。这一行指定了解释器的路径，告诉系统应该使用哪个Shell或者其他解释器来执行脚本。 #!/bin/bash ** 运行脚本的方式？ bash dh.sh sh dh.sh source dh.sh . dh.sh 路径 绝对路径 dh.sh 相对路径 dh.sh 路径执行的话，该脚本文件必须有可执行的权限 X 路径其实底层就是调的bash来运行脚本的 区别： bash 或 sh 执行脚本时会新开一个 bash（进程），不同 bash 中的变量无法共享。 而 source 或 . 是在同一个 bash 里面执行的，所以变量可以共享。 Shell 脚本必须以 .sh 后缀结尾吗 # ​\t不必须以.sh后缀结尾，只要里面的内容是shell语言就行。\nLinux 查看进程的命令以及杀死进程的命令 # 查看进程命令： ps -ef 查看所有进程列表 ps aux ps -aux 显示所有用户的所有进程的详细信息。 top 进程监控工具，可以实时显示系统中最消耗资源的进程。按 q 键退出 top。 杀死进程的命令： kill -9 进程id 强制终止进程 kill -15 进程id 优雅的终止 ZooKeeper # ZooKeeper 集群中有哪些角色，分别有什么作用 # Leader（领导者）\nLeader 是集群中处于活动状态的角色，负责处理来自客户端的写请求。 Leader 负责在集群中广播客户端的更新，确保所有Follower节点的数据一致性。 它还负责定期向Follower和Observer节点发送心跳消息，以维持集群的正常运作。 Leader 也处理来自客户端的读请求，但为了提高效率，读请求也可以由Follower处理。 Follower（跟随者）\nFollower 节点主要处理客户端的读请求 参与事务请求Proposal 的投票参与 Leader 选举投票。 转发事务请求给 Leader 服务器 Observer（观察者）\nObserver 类似于Follower，但它不参与事务请求 Proposal 的投票，也不参与 Leader 选举投票。\nObserver 只提供非事务的服务（查询）\u0026ndash;处理读请求，不参与写操作\n说说 ZooKeeper Znode 的特点 # ZooKeeper 中的 znode（ZooKeeper node）是 ZooKeeper 数据模型的基本构建单元，它类似于文件系统中的文件和目录。znode 具有以下特点：\n树形结构：\nznode 形成一个类似文件系统的层次结构，每个 znode 可以拥有子 znode，最顶层的根节点为 /。 znode 的路径是唯一的，路径格式为 /path/to/znode。 持久化（Persistent）与临时（Ephemeral）：\n持久化 znode：一旦创建，就会一直存在，除非显式删除。 临时 znode：这类 znode 的生命周期与创建它的会话绑定。当会话结束时，所有由该会话创建的临时 znode 将被自动删除。 顺序性（Sequential）：\n创建 znode 时可以选择添加顺序属性，这样创建的 znode 名称后面会附加一个递增的数字，确保了创建顺序的唯一性。例如，如果创建了一个名为 /test 的顺序 znode，可能生成的节点名为 /test0000000001。 版本控制：\n每个 znode 都有版本号，包括数据版本（dataVersion）和 ACL 版本（aclVersion），用于跟踪数据和权限的变化。 原子性：\n对于 znode 的读写操作是原子的，要么完全完成，要么完全不发生。 数据大小限制：\n单个 znode 存储的数据大小有限制，通常不超过 1MB。 事件通知：\n客户端可以注册监听器来接收 znode 的状态变化通知，如创建、删除、数据修改等。 ACLs (Access Control Lists)：\n每个 znode 都有自己的访问控制列表，用于定义哪些客户端可以对该 znode 执行何种操作。 快照和日志：\nZooKeeper 为每个 znode 维护快照和事务日志，以确保数据的一致性和恢复能力。 唯一性：\n由于 ZooKeeper 提供了顺序 znode 和临时 znode，它常被用于实现分布式锁、队列、选主等场景。 这些特性使 ZooKeeper 成为一种强大的协调服务，广泛应用于分布式系统中解决一致性问题，如配置管理、服务发现、命名服务、集群管理等。\n说说 ZooKeeper 的监听通知机制 # ZooKeeper 的监听通知机制是其关键特性之一，它允许客户端对特定类型的事件进行注册监听，从而在这些事件发生时得到通知。这种机制对于构建高可用和响应式的分布式系统至关重要。下面是 ZooKeeper 监听器的工作原理和主要概念：\n监听器注册： 当客户端连接到 ZooKeeper 服务器后，它可以注册监听器来监听特定的事件。监听器可以注册在单个 znode 上，也可以在客户端的整个会话上注册。\n事件类型： ZooKeeper 支持以下几种事件类型，客户端可以通过监听器接收到这些事件的通知：\nNodeCreated：当一个新的 znode 被创建时。 NodeDeleted：当一个 znode 被删除时。 NodeDataChanged：当一个 znode 的数据发生变化时。 NodeChildrenChanged：当一个 znode 的子节点列表发生变化时。 SessionEvent：会话级别的事件，包括会话建立、断开、过期等。 事件通知： 当上述事件发生时，ZooKeeper 会发送一个异步的事件通知给注册了监听器的客户端。这个通知包含事件的类型以及触发事件的 znode 的路径。\n监听器一次性： 重要的是要注意，ZooKeeper 的监听器是一次性的。这意味着每当一个事件发生并通知了客户端之后，对应的监听器就会被清除。如果客户端想要继续接收事件通知，它需要重新注册监听器。\n事件处理： 客户端在接收到事件通知后，可以采取相应的行动，比如更新本地缓存，重新获取数据，或者调整应用程序的行为。\n事件顺序： ZooKeeper 保证事件通知的顺序与事件发生的顺序一致，这有助于保持数据的一致性。\n监听通知机制使得 ZooKeeper 成为了构建分布式应用的理想选择，因为它允许应用程序在数据变化时快速做出反应，而无需频繁轮询数据。这对于实现如服务发现、分布式锁、配置管理等功能非常重要。\n说一下 CAP 原则以及如何选择 # CAP原则 # CAP 原则是分布式系统设计中的一个核心概念，由加州大学伯克利分校的 Eric Brewer 教授提出，并由 Seth Gilbert 和 Nancy Lynch 从理论上证明。CAP 原则指出，在分布式计算中，任何一个网络分区容忍的系统只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三个需求中的两个，不可能同时达到三个目标。\n具体来说：\n一致性（Consistency）：在数据写入成功后，所有的读操作都会返回最新的数据，即所有节点在同一时刻看到相同的数据视图。 可用性（Availability）：每个请求无论数据一致性如何，都应该在合理的时间内获得响应。这意味着系统必须保证非故障节点的持续服务，即使在部分节点故障的情况下。 分区容错性（Partition tolerance）：系统应该能够在网络分区的情况下继续工作，即即使一部分网络不可用，系统仍能正确地处理请求。 如何选择 # 在设计分布式系统时，根据具体的应用场景和需求，通常需要在 CAP 三者之间做出权衡。以下是一些选择策略：\nCA 系统：选择一致性和可用性，放弃分区容错性。这种系统假设网络是稳定的，没有分区情况出现。传统的关系型数据库和事务处理系统倾向于采用 CA 设计，它们在本地网络环境下提供强一致性和高可用性。\nCP 系统：选择一致性和分区容错性，牺牲可用性。在检测到网络分区时，系统会选择暂停服务或拒绝写入请求，直到分区恢复，以保持数据的一致性。例如，Google Spanner 和 Apache Cassandra 在某些配置下可以看作是 CP 系统。\nAP 系统：选择可用性和分区容错性，牺牲一致性。在检测到网络分区时，系统会继续提供服务，但允许不同节点间的临时不一致性，直到网络分区恢复。NoSQL 数据库如 Amazon DynamoDB 和一些分布式缓存系统倾向于 AP 设计。\n实际应用中，很多系统会采用混合策略，例如最终一致性（eventual consistency），在牺牲短暂一致性的前提下提供高可用性和分区容错性。设计时需要仔细考虑业务需求，比如实时性、数据完整性、用户体验等因素，来决定如何平衡 CAP 之间的关系。\nZooKeeper 的选主过程 # ZooKeeper的选主过程是其确保分布式数据一致性和集群稳定性的关键机制。选主过程通常发生在两种场景下：集群初始化时和Leader不可用（如崩溃或失去连接）时。以下是ZooKeeper选主过程的详细步骤：\n一、集群初始化时的选主过程 # 节点注册： 集群中的每个节点（Server）都会向ZooKeeper注册，并声明自己的身份（如myid）和状态。 初始化选主阶段： 当集群启动时，ZooKeeper会进入一个初始化的选主阶段。所有节点都会尝试与ZooKeeper建立连接，并发送选主请求。这些请求使用心跳信号来确认节点的存活性和连通性。 发送投票： 在初始化阶段结束后，每个节点会根据其意愿选择一个领导者节点，并向集群中的其他节点发送投票。投票信息通常包含节点的SID（Server ID）和ZXID（ZooKeeper事务ID）。 处理投票： 每个节点收到其他节点的投票后，会进行投票的PK。PK的规则主要是： 优先检查ZXID，ZXID较大的节点优先作为Leader。 如果ZXID相同，则比较SID，SID较大的节点作为Leader。 统计投票： 节点会统计收到的投票信息，判断是否已经有过半的节点投出了相同的票。 选举结果通知： 当某个节点的票数超过半数时，该节点被选为Leader。ZooKeeper会将选举结果通知给所有节点。 切换为主模式： 一旦Leader节点被选举出来，集群将进入主模式。Leader节点负责处理关键决策和协调集群的操作，其他节点则作为Follower与Leader保持同步。 二、Leader不可用时的选主过程 # 变更状态： 当Leader节点发生故障或失去连通性时，剩余的Follower节点会将自己的状态变更为LOOKING，并开始进入新一轮的Leader选举过程。 发送投票： 与集群初始化时类似，每个Follower节点都会发送自己的投票，并接收来自其他节点的投票。 处理投票： 节点根据PK规则处理收到的投票，并更新自己的投票（如果需要）。 统计投票： 统计投票信息，判断是否有节点获得了过半的票数。 选举结果通知： 当新的Leader节点被选举出来时，ZooKeeper会通知所有节点。 同步与恢复： 新选举出的Leader节点会与Follower节点进行状态同步，确保集群的一致性。 三、选主过程中的关键要素 # SID（Server ID）：节点的唯一标识符，用于区分集群中的不同节点。 ZXID（ZooKeeper事务ID）：ZooKeeper状态的每一次改变都会对应一个递增的事务ID，用于判断节点的数据新旧程度。 LOOKING状态：节点在寻找Leader时的状态，表示当前集群中没有Leader或Leader不可用。 LEADING状态：节点作为Leader时的状态，负责处理集群的关键决策和协调操作。 FOLLOWING状态：节点作为Follower时的状态，与Leader保持同步并遵循其指示。 通过这一系列的步骤和规则，ZooKeeper能够确保在集群初始化或Leader不可用时，快速、稳定地选举出新的Leader节点，从而维持集群的正常运行和数据的一致性。\n第一次选主：\n1 无脑选议员编号最大的为主 2 票数过半才能成为主 3 投票默认投票给议员编号（myid）最大的节点 4 每次选主都会有一个朝代的标记 5 每次选主成功后，其他节点需要和主进行数据同步\n主宕机后重新选主：\n1 先过滤出公共pid相同的节点 2 执行第一次选主的步骤\nPaxos算法最终模型：奇数台服务器+有主模式+角色\nZooKeeper 如何帮助其他组件选主 # ZooKeeper作为一个分布式的、开放源码的分布式应用程序协调服务，它通过其独特的机制和特性来帮助其他组件完成选主过程。以下是ZooKeeper如何帮助其他组件选主的详细解析：\n1. 提供注册与监听机制 # 节点注册：ZooKeeper允许其他组件中的每个节点（或称为服务实例）在ZooKeeper中注册一个唯一的临时节点（Ephemeral Node）。这个节点会随着服务实例的启动而创建，随着服务实例的停止而删除。 监听机制：ZooKeeper通过监听机制，允许组件中的其他节点监听特定节点的状态变化。这种监听可以是节点创建、节点删除或节点数据更新等事件。 2. 实现选主流程 # 初始化选主：当集群启动时或Leader节点失效时，ZooKeeper会触发选主流程。每个节点（或称为参与者）都会向ZooKeeper发送自己的选主请求，这些请求中包含了节点的身份信息和状态信息。 投票与统计：ZooKeeper负责接收并统计这些选主请求。每个节点都会根据一定的规则（如节点的ZXID和SID）进行投票，并将投票结果发送给ZooKeeper。ZooKeeper会统计每个候选节点的票数，当某个节点的票数超过半数时，该节点就被选为Leader。 结果通知：一旦选举出Leader，ZooKeeper会将选举结果通知给集群中的所有节点。其他节点会根据选举结果调整自己的状态，并与新的Leader节点建立连接。 3. 保证数据一致性和稳定性 # 数据同步：在选举出Leader后，Leader会负责协调集群中的数据同步。它会确保集群中每个节点上的数据都是最新的，并且保持一致。 故障转移：如果Leader节点发生故障或失去连接，ZooKeeper会自动触发故障转移过程。其他节点会重新开始选举流程，并选举出新的Leader节点。这个过程是自动的，并且能够快速完成，从而保证了集群的稳定性和可用性。 4. 选举算法 # ZooKeeper的选主过程通常基于Paxos算法或其变种（如FastLeaderElection算法）。这些算法能够保证在分布式环境中选举出唯一的Leader节点，并且能够保证选举过程的一致性和可靠性。\n5. 示例场景 # 假设有一个分布式服务集群，该集群中的每个服务实例都需要一个Master节点来处理关键决策。这些服务实例可以使用ZooKeeper来帮助完成Master选举过程。它们会在ZooKeeper中注册自己的节点，并监听特定节点的状态变化。当需要选举Master时，它们会向ZooKeeper发送选主请求，并根据ZooKeeper的统计结果来确定哪个服务实例被选为Master。\n总之，ZooKeeper通过其注册与监听机制、选主流程、数据同步和故障转移等特性来帮助其他组件完成选主过程。这些特性使得ZooKeeper成为分布式应用程序协调服务中的佼佼者。\nHadoop # HDFS 的读取流程 # 客户端向NameNode发起文件读取请求-\u0026gt;NN检查相关权限响应检查结果给客户端-\u0026gt;客户端询问NN Block从哪里读取-\u0026gt;NN通过集群拓扑结构告知客户端要读取的Block数据在哪些DataNode上-\u0026gt;客户端向这些DN请求建立连接传输通道-\u0026gt;客户端开始读取数据\nHDFS 的写入流程 # 客户端向NameNode发起文件写入请求-\u0026gt;NN检查相关权限响应检查结果给客户端-\u0026gt;客户端询问NN Block上传到哪-\u0026gt;NN通过机架感知策略告知客户端写入哪些DataNode上-\u0026gt;客户端向这些DN请求建立连接传输通道-\u0026gt;客户端开始写数据\n阐述 MapReduce 的计算流程 # Block块切片 MapTask：读取切片数据 读入环形缓存区 分区排序（Hash分区快排）溢写到磁盘 （可选预聚合）合并\nReduceTask：从MapTask拉取数据到内存缓存区 溢写 （可选预聚合）合并 分组 计算完后把数据存到HDFS、DB、NoSql\n阐述 YARN 集群的工作流程 # 为什么会产生 YARN 它解决了什么问题 # 总的来说就是为了更好的资源管理和作业调度。\n在早期的Hadoop版本中，MapReduce框架同时充当了资源管理和作业调度的角色，导致资源管理和作业调度的逻辑紧密耦合在一起。 MapReduce框架以作业为单位进行资源调度，可能出现某个作业占用大量资源但处理速度较慢，而其他作业需等待较长时间才能开始执行，导致资源利用率下降。 解决问题\n多调度器支持：YARN将资源管理和作业调度等功能从MapReduce中拆分出来，形成了一个独立的资源管理平台。YARN支持多种调度器，如容量调度器（Capacity Scheduler）、公平调度器（Fair Scheduler）等，可以根据需求选择合适的调度算法和策略，实现对集群资源的更精细管理和调度。 分离计算框架：YARN将资源管理和作业调度与具体的计算框架解耦，不再限定于MapReduce。这使得Hadoop生态系统可以支持多个计算框架，如Spark、Hive、Flink等。用户可以选择最适合自己需求的计算框架，提高了灵活性和可用性。 高效的资源利用：YARN引入了容器（Container）的概念，将物理机的资源划分为多个容器，每个容器可以执行一个任务。这种隔离的机制使得资源可以更加细粒度地分配和利用，避免作业间的资源竞争，提高了资源利用率。 ​\nHadoop MR 模型中数据倾斜一般是在 Mapper 端发生的还是在 Reducer 端发生的，为什么 # 一般是在Reduce端\n数据分组与排序：Mapper输出的数据会根据Key进行分组，并发送到相应的Reducer进行处理。如果某些Key的数据量显著大于其他Key，那么处理这些Key的Reducer就会收到更多的数据，导致处理时间延长，形成数据倾斜。 Shuffle阶段：在Shuffle阶段，Mapper输出的数据会被排序和传输到Reducer。由于数据是按照Key进行排序和分组的，因此具有相同Key的数据会被发送到同一个Reducer，这进一步加剧了数据倾斜的可能性。 Reduce数量固定：在MapReduce作业中，Reducer的数量是事先设定的，且在整个作业执行过程中保持不变。这意味着即使某些Key的数据量很大，它们也只能被发送到有限的几个Reducer上进行处理，从而加剧了数据倾斜问题。 自定义Partitioner：如果使用了自定义的Partitioner来分配数据到Reducer，不恰当的Partitioner设计也可能导致数据倾斜。例如，如果Partitioner不能将数据均匀地分配到各个Reducer上，就可能导致某些Reducer处理的数据量远大于其他Reducer。 为了缓解Reducer端的数据倾斜问题，可以采取以下策略：\n增加Reducer数量：通过增加Reducer的数量，可以将数据更细粒度地分布在更多的Reducer上，从而减轻单个Reducer的负载。 优化Partitioner：使用更合理的Partitioner来分配数据到Reducer，确保数据能够尽可能均匀地分布。 使用Combiner：在Mapper端使用Combiner进行局部聚合，减少输出到Reducer端的数据量，从而降低数据倾斜的可能性。 数据预处理：在数据处理前对数据进行预处理，如过滤掉异常值、对数据进行重新排序或分组等，以减少数据倾斜的发生。 Hadoop 常用的压缩算法有哪些，有什么区别 # Hadoop MR 模型中哪些地方可以进行优化 # 数据输入阶段\n合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。可以采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 Map阶段\n避免使用正则表达式：Map阶段如果大量使用正则表达式处理数据，可能会增加CPU负担，降低处理速度。 优化字符串处理：在处理字符串时，使用性能更高的工具类，如StringUtils，避免使用String、Scanner、StringTokenizer等效率较低的工具。 使用Combiner：在Map之后、Reduce之前引入Combiner进行局部聚合，可以减少传输到Reduce端的数据量，提高处理效率。 合理设置Map数量：Map数量过多或过少都会影响作业性能，应根据输入数据量和集群资源情况合理设置。 Shuffle和Sort阶段\n数据压缩：对Map输出的数据进行压缩，可以减少网络传输的数据量，但会增加CPU成本，应权衡利弊。 调整Shuffle参数：如调整io.sort.mb和sort.spill.percent等参数，可以增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。 Reduce阶段\n合理设置Reduce数量：Reduce数量过多或过少都会导致性能问题，应根据数据量和集群资源情况合理设置。 使用自定义Partitioner：如果默认Partitioner不能将数据均匀分配到各个Reducer上，可以编写自定义Partitioner来改善数据分布。 减少合并次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。 IO和存储优化\nHDFS优化：包括卷选择策略、元信息存储路径设置等，确保HDFS的高效运行。 使用紧凑二进制文件格式：如Avro、SequenceFile等，而不是文本文件，可以减少IO和网络开销。 参数调优\nHDFS参数调优：如调整dfs.namenode.handler.count、dfs.namenode.edits.dir等参数，优化NameNode的性能。 YARN参数调优：如调整yarn.nodemanager.resource.memory-mb、yarn.scheduler.maximum-allocation-mb等参数，合理分配集群资源。 MapReduce参数调优：如调整mapreduce.reduce.input.buffer.percent等参数，优化Reduce端的性能。 其他优化\n避免不必要的Reducer：如果Map输出数据可以直接使用，无需进行Reduce操作，则可以避免使用Reducer，减少处理时间和资源消耗。 使用Snappy或LZO等高效压缩算法：对数据进行压缩，减少网络传输和存储的开销。 Hive # Hive SQL 的执行流程 # 解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，对AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。 编译器（Compiler）：将 AST 编译生成逻辑执行计划。 优化器（Query Optimizer）：对逻辑执行计划进行优化。 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。最后执行。对于 Hive 来说，就是 MR/Spark。 Hive 自定义函数的流程，有什么作用 # 首先自定义函数分为三种：\nUDF（User Defined Function）：普通函数，一进一出，比如 UPPER, LOWER UDAF（User Defined Aggregation Function）：聚合函数，多进一出，比如 COUNT/MAX/MIN UDTF（User Defined Table Generating Function）：表生成函数，一进多出，比如 LATERAL VIEW EXPLODE()侧视图行转列 流程：\n编写Java代码继承相应的类或实现相应的接口，重写父类的方法 将自定义函数程序打成 jar 包并上传至 HDFS 在 Hive 中根据上传的jar包注册自定义函数，并指定函数名。 重新加载函数。 就能在Hive SQL中像使用内置函数一样使用它啦 作用：\n扩展 Hive 的计算能力：Hive 自身已经提供了非常丰富的内置函数，但是数量与功能有限，在某些特定场景下，如果无法满足需求，可以通过自定义函数来进行功能的扩展。 提高数据处理效率：对于需要重复执行的复杂数据处理逻辑，将其封装为自定义函数可以提高查询的执行效率。 便于复用和维护：将常用的数据处理逻辑封装为自定义函数后，可以在多个查询中复用，减少了代码的冗余。同时，当数据处理逻辑需要更新时，只需修改自定义函数的实现，而无需修改所有使用该逻辑的查询。 说一说 Hive 的分区分桶 # 讲Hive分区分桶之前，我先说一下Hive的数据模型，主要有以下四种：\n单独的一个表； 对表进行分区； 对表进行分区，再分桶； 对表直接分桶。 在大数据中，最常见的思想就是分而治之，把大文件分割成一个个的小文件，对小文件操作时会容易很多，同样，在Hive中也是支持的，我们可以把大的数据，按照每天或者每小时切分成一个个的小文件，这样去操作小文件就会容易许多，这就是分区、分桶的意思。\n分区：\n分区就相当于在HDFS上创建了一个文件夹，把数据放到文件夹中。 根据分区深度可以分为：单分区和多分区。 根据分区类型可以分为：静态分区和动态分区。区别在于：静态分区是手动指定，而动态分区是通过数据来判断分区的。动态分区下可以设置是否开启严格模式，默认非严格模式，严格模式主要禁止三种类型的查询，当进行这三种类型的查询时就会报错。 使用分区，可以避免 Hive 全表扫描，能够指定分区查询数据，提升查询效率； 使用分区表时，尽量利用分区字段进行查询，如果不使用分区字段查询，就会全部扫描，这样就失去了分区的意义。 分桶：\n并不是所有的数据都可以形成合理的分区的，不合理的分区方式可能会导致有的分区数据过多，而有的分区没什么数据的情况。所以就有了分桶，分桶是更加细粒度的数据范围划分，把数据按照字段划分到多个文件中去。 分桶原理：对分桶字段Hash，然后除以桶的个数求余，来确定放哪个桶。 分桶优势：方便抽样、提高JOIN查询效率。 使用分桶表时，尽量利用分桶字段进行查询，如果不使用分桶字段查询，就会全部扫描，这样就失去了分桶的意义。 分区针对的是数据的存储路径，分桶针对的是数据文件。\nHive 和传统数据库的区别 # 查询语言 Hive使用类似于SQL的查询语言，叫做HiveQL 传统的数据库通常使用标准的SQL进行查询 数据存储位置 Hive依赖于Hadoop，其数据存储在Hadoop的分布式文件系统HDFS中。 传统数据库存储在块设备或本地文件系统中。 数据格式 Hive中没有定义专门的数据格式，由用户指定，需要指定三个属性：列分隔符，行分隔符，以及读取文件数据的方法。 传统数据库中，存储引擎定义了自己的数据格式。 数据更新 Hive不支持数据的修改操作 数据库支持对数据的频繁修改，包括添加、删除和更新等操作。 索引 Hive废弃了索引 传统数据库会建立索引 执行方式 Hive底层用的是MapReduce 数据库是执行引擎（Execution Engine） 可扩展性 Hive扩展性高 传统数据库扩展性低 数据规模 Hive为了处理大规模数据集设计的，能够处理PB级别的数据。 传统数据库虽然也支持，但随着数据量的增加，性能会下降。 Hive 的物化视图和视图有什么区别 # Hive的视图是一种虚拟表，只保存定义，不会存储数据。用来简化SQL操作的，不能提高查询速度。\n物化视图是把复杂的操作先查出来，然后存到物化视图里面，能够提高查询的速度。物化视图只可以在事务表上创建。\nHive 内部表和外部表的区别 # 建表时默认创建的就是内部表，外部表通过EXTERNAL关键字创建。\n创建内部表时会在HDFS中创建对应的目录，载入数据时，对应的数据文件会移动到表所在的目录下。\n外部表创建时指定数据位置【推荐，数据不会随意移动】，不会在HDFS中创建对应的表目录。\n外部表先建表再载入数据，会在HDFS中创建对应的表目录，载入数据时，对应的数据文件会移动到表所在的目录下。\n删除内部表时， 真实数据和表对应的目录也一并删除。删除外部表时，只会删除元数据，真实数据和表对应的目录不会被删除。\nHive 导出数据有几种方式 # 通过 SQL 操作 将查询结果导出到本地 INSERT OVERWRITE LOCAL DIRECTORY \u0026lsquo;/root/user\u0026rsquo; SELECT * FROM t_user; 将查询结果导出到 HDFS INSERT OVERWRITE DIRECTORY \u0026lsquo;/yjx/export/user\u0026rsquo;。。。。。 通过 HDFS 命令操作 就是把HDFS中的数据复制一份 hdfs dfs -cp /hive/warehouse/t_person/* /yjx/export/person 将元数据和数据同时导出 EXPORT TABLE t_person TO \u0026lsquo;/yjx/export/person\u0026rsquo;; 将表结构（元数据）和数据同时导出 Hive 动态分区和静态分区有什么区别 # 创建方式 静态分区是用户在创建表时手动指定分区的列。 动态分区则是在加载数据时根据数据的值自动生成分区，分区的值是根据数据内容动态确定的。 使用场景 静态分区适用于数据量小且变化不频繁的场景 动态分区适用于数据量大且频繁变化的场景 配置 静态分区没有什么配置要求，建表时指定分区字段就行 动态分区需要在Hive中进行相关参数的配置 Hive 的 sort by、order by、distrbute by、cluster by 的区别 # 这是Hive 中的四种排序：\n全局排序：ORDER BY 全局排序是在一个 Reduce 中进行全局数据的排序。 内部排序：SORT BY 只在每个 Reducer 内部进行排序。 分区排序：DISTRIBUTE BY DISTRIBUTE BY 会根据指定的字段将数据分到不同的 Reducer，且分发算法是 Hash 散列。一般结合 SORT BY 使用（注意：DISTRIBUTE BY要在 SORT BY 之前）。 组合排序：CLUSTER BY CLUSTER BY 既有 DISTRIBUTE BY 的功能外还有 SORTS BY 的功能，当 DISTRIBUTE BY 和 SORTS BY 字段相同时，可以使用 CLUSTER BY =DISTRIBUTE BY+SORTS BY。但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC，否则会报错： 行式文件和列式文件的区别 # 数据的存储方式 行式：将数据按行存储在磁盘上 列式：将数据按列存储在磁盘上 查询效率 行式：查询时需要读取整行数据，即使只需要查询很少部分列，导致不必要的I/O开销 列式：查询时只需读取所需的列，减少了I/O开销，查询效率高 压缩效率 行式：同一行数据中，列的数据类型不一致，压缩比低，压缩效率低。 列式：同一列中数据类型一样，压缩比高，压缩效率高。 说一说常见的 SQL 优化 # RBO优化\n谓词下推 靠近数据源的位置提前过滤掉一些无关的数据 列裁剪 扫描数据源时，只读取所需的字段 常量替换 将不变的表达式提前计算出结果，用该结果把表达式替换掉 CBO优化\n特别适用于处理包含JOIN操作的复杂查询，它能计算所有可能执行计划的代价，并挑选出代价最小的执行计划。。 JOIN优化\nMap Join\t顾名思义，就是在 Map 阶段进行表之间的连接。 应用场景：小表 JOIN 大表或者小表 JOIN 小表。 实现原理：Map Join 会把小表全部读入内存中，在 Map 阶段直接拿另外一个表的数据和内存中的表数据做匹配，由 于在 Map 阶段进行了 JOIN 操作，底层不需要经过 Shuffle，这样就不会由于数据倾斜导致某个 Reduce 上落数据太多而失败，但是需要占用内存空间存放小表数据。 Hive 中默认开启了 Map Join Reduce Join 应用场景：大表 JOIN 大表。 实现原理：将两张表的数据在 Shuffle 阶段利用 Shuffle 的分组将数据按照关联字段进行合并。 Bucket Join 大表JOIN大表 大表数据分桶，桶与桶之间进行JOIN，减少了比较次数，避免全表扫描。 Bucket Map Join Sort Merge Bucket Join（SMB Join） 根据现阶段所学知识总结一下产生数据倾斜的原因 # 本质来说有两种原因：\nTask读取大文件，读取压缩的不可分割的大文件。 任务需要处理大量相同键的数据。 单表聚合操作，部分 Key 数据量较大，且大 Key 分布在很多不同的切片。 两表进行 JOIN，都含有大量相同的倾斜数据键。 数据含有大量无意义的数据，例如空值(NULL)、空字符串等。 含有倾斜数据在进行聚合计算时无法聚合中间结果，大量数据都需要经过 Shuffle 阶段的处理，引起数据倾斜。 数据在计算时做多维数据集合，导致维度膨胀引起的数据倾斜。 说一说 Hive 你知道的优化（问的几率不大，时间充足可以看看） # SQL优化 资源优化 向量化查询 存储优化 YARN 优化 并行执行 JVM 重用 聚合优化 GROUP BY 优化 ORDER BY 优化 COUNT(DISTINCT) 优化 Job 优化 Map 优化 Reduce 优化 Shuffle 优化 HBase # RowKey 如何设计，设计不好会产生什么后果 # HBase 的 RowKey 设计应当遵循以下原则：以确保数据的均衡分布和高效访问。\n唯一原则 因为HBase中的数据是以键值对的形式存储的，相同的RowKey会覆盖旧的数据。 RowKey 是按字典序排序存储的，设计 RowKey 时，要充分利用这个排序的特点，将经常一起查询的数据放到相近的RowKey上。 单主键 组合主键 长度原则 不要超过 16 个字节 RowKey 是一个二进制码流，最大长度是 64KB，建议越短越好。不要超过 16 个字节。数据的持久化文件 HFile 中是按照 KeyValue 存储的，如果数据量特别大，RowKey过长的话，HFile文件就会占用很大的存储空间，HFile 的存储效率降低。MemStore 将缓存数据到内存时，如果 RowKey 字段过长，内存不够，无法缓存更多的数据，会降低检索效率。 对齐 RowKey 长度 根据不同的场景，在有些情况下，还需要对齐 RowKey 的长度：因为 RowKey 是按字典序排列的，比如 ID 取 12 位，9 位 ID 前就需要补齐 3 个 0，否则就会出现 123456789 比 654321 排在前面的问题。对齐长度后，000000654321 就会排在 000123456789 之前，符合预期。 散列原则 反转 加盐（添加随机前缀） Hash 可能产生的后果：\n数据倾斜与热点问题： 如果RowKey设计不当，导致大量数据集中在少数几个RegionServer上，就会出现数据倾斜和热点问题。 会使得这些RegionServer的负载过高，影响查询性能，甚至可能导致节点崩溃。 查询效率低： 可能会使得相关的查询操作需要扫描大量的数据行才能找到目标数据。 增加了查询的延迟，还可能对集群的整体性能造成影响。 存储效率低： 过长的RowKey会占用更多的存储空间，降低存储效率。 过长的RowKey还可能导致内存的利用率下降。 管理复杂性增加： 设计不合理的RowKey会使得数据的分布变得难以预测和控制，增加了数据管理和维护的复杂性。 列族如何设计，为什么不建议 HBase 设计过多列族 # 追求的原则是：在合理的范围内，尽可能的减少列族\n怎么设计呢？\n如果某些列经常被一起查询，那么可以把这些列放在同一个列族中。查询效率高，减少了访问不同的磁盘文件。 减少列族的数量，官方推荐\u0026lt;=3个 列族名的长度要尽量小，节省空间，加快效率，最好是一个字符，比如 d 表示 data 或default，v 表示 value。 为什么不建议 HBase 设计过多列族？\n内存和IO消耗：每个列族对应Region中的一个Store，每个Store又包含一个MemStore。当MemStore达到一定阈值时，会将其中的数据flush到磁盘上的StoreFile中。如果列族数量过多，会导致每个Region中的MemStore数量增多，进而增加了需要flush的数据量。这不仅会影响flush的IO性能，还会消耗更多的集群资源，甚至可能引发memstore的flush阻塞现象。 数据分布和查询效率：如果每个列族的数据量分布不均匀，比如某些列族的数据量远大于其他列族，那么在Region分裂时，可能会导致数据量小的列族在每个Region中的数据量过少。这会导致查询这些小数据量列族时，需要横跨多个Region，从而降低查询效率。 HBase 读取数据的流程 # Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 HRegionServer。\n访问对应的 HRegionServer，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey，查询出目标数据位于哪个 HRegionServer 中的哪个 HRegion 中。（并将该 table 的 HRegion 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问）。\n向HRegionServer发送读数据请求与目标 HRegionServer 建立连接；\n分别在 BlockCache（读缓存）-\u0026gt;MemStore （写缓存）-\u0026gt;Store File（HFile）中层层往下查询目标数据。\n将查询到的数据，先缓存到BlockCache。\n然后再将数据返回给客户端。\nHBase 写入数据的流程 # Client 访问 ZooKeeper，获取 hbase:meta 所在 HRegionServer 的节点信息；\nClient 访问 hbase:meta 所在的 HRegionServer，获取 hbase:meta 记录的元数据后先加载到内存中，然后再从内存中查询出 RowKey 所在的 HRegion （HRegion 所在的 HRegionServer）；（写入数据的RowKey，根据RowKey查询（按字典排序）出其应该被分在哪个HRegion）。\nClient 对 RowKey 所在的 HRegion 对应的 HRegionServer 发起写入数据请求；\n建立连接后，首先将 DML 要做的操作写入到日志 HLog；\n然后将数据的修改更新到 MemStore 中，本次操作结束：一个 HRegion 由多个 Store 组成，一个 Store 对应一个列族，Store 包括位于内存中的 Memstore 和位于磁盘的 StoreFile，写操作先写入 MemStore；\n当 MemStore 数据达到阈值后（默认 128M），创建一个新的 MemStore；\n旧的 MemStore 将刷写为一个独立的 StoreFile（HRegionServer 会启动 FlushCache 进程写入 StoreFile）并存放到HDFS，最后删除 HLog 中的历史数据。\n当 StoreFile 文件的数量增长到一定阈值后，系统会进行合并（次/小 Minor Compaction、主/大 Major Compaction）；\n在合并过程中会进行版本合并和删除工作，形成更大的 StoreFile；\n当一个 HRegion 所有 StoreFile 的大小和数量超过一定阈值（10G）后，会把当前的 HRegion 分割为两个，并由 HMaster 分配到相应的 HRegionServer 服务器，实现负载均衡。\nHive 和 HBase 的区别？？？？？ # Hive： Hive并不是数据库，是一个基于 Hadoop 的数据仓库工具 Hive本身不存储数据，真正的数据存储在HDFS中，它完全依赖HDFS和MapReduce，Hive通过元数据会把这些数据映射成一张数据库表（逻辑表）。支持SQL查询，本质是将 HQL 转化成 MapReduce 程序。 Hive使用与对结构化数据进行查询分析 主要是支持批处理数据。不擅长处理实时数据更新。 HBase： HBase是一种基于HDFS的分布式、面向列的非关系型数据库。键值对的方式存储数据。表是物理表。 HBase的数据模型是面向列的，表由行键（Row Key）、时间戳（Timestamp）（数据version）、列族（Column Family）和列（Column）组成。 系统命名空间hbase中有元数据表(meta)（存了整个HBase中的所有其他的表的【数据与region，以及region与RegionServer的映射】）\u0026ndash;通过该机制能快速定位到数据。提高读写效率。 HBase 的主要作用就是面向 PB 级别数据的实时入库和快速随机访问 支持实时处理数据。（秒级查询\u0026ndash;实时）高吞吐量，低延迟。 为什么要使用 Phoenix # HBase作为一种原始的键值存储数据库，它的数据模型是非常简单的，它提供的主要操作是通过行键（Row Key）进行数据的读写。对于简单的查询和数据的增删改查操作，HBase已经足够强大。但是当我们需要进行类似传统SQL数据库中的复杂查询、聚合、排序等操作时，HBase就显得力不从心了。这是因为HBase并没有像传统SQL数据库一样提供索引以及查询优化的功能。 Phoenix为HBase提供了执行SQL查询的接口，它基于SQL标准，并且提供了许多SQL数据库中常见的特性，如二级索引、事务等，我们可以通过简单的SQL语句直接查询HBase中的数据，而无需编写复杂的HBase的Java API代码，和用HBase Shell命令来操作HBase数据库，提高了开发效率。 Phoenix将SQL查询转换为HBase的Scan扫描操作来实现高效的查询。在HBase上构建了一套索引机制，使得查询更加高效。 Apache Phoenix还支持将查询结果直接写入到HBase中。 HBase 的热点区域会产生什么问题 # 性能下降 当热点区域出现时，意味着大量的读写请求集中在少数几个Region Server上。这些Region Server会承受远高于其他节点的负载压力，导致处理速度变慢，响应时间变长。 在高负载持续作用下，过载的Region Server可能会出现性能瓶颈，甚至因资源耗尽而崩溃，导致部分或全部服务不可用。这不仅会影响用户体验，还可能对业务造成严重影响。 由于热点区域的数据访问过于集中，查询操作可能需要等待很长时间才能获得响应，查询效率就会降低。 资源利用不均衡 在HBase集群中，如果大部分数据都集中在少数几个Region Server上，而其他节点则处于空闲或低负载状态，那么就会造成硬件资源的浪费。这不仅增加了运营成本，还降低了集群的整体效率。 HBase的Region Server本应具有负载均衡的能力，通过合理的数据分布来平衡各节点的负载。然而，热点区域的出现打破了这种平衡，使得负载均衡机制失效。 说一说 HBase 的数据刷写与合并 # 刷写 刷写是HBase将数据从内存中的MemStore写入到磁盘上的HFile文件的过程 刷写触发时机： 内存阈值： HRegion级别的刷写。Memstore达到128M触发刷写，把数据写出到HFile。情况：Memstore达到128M，但是还有数据一直往MemeStore里面写\u0026mdash;当MemStore达到128M时，先上锁，再拍快照，然后往外刷，如果出问题，就重新刷，这时刷快照里面的，直到不出问题，就释放锁。当数据写入的非常快时，MemStore还没刷写完，锁还没释放，就会再生成MemStore\u0026ndash;最多四个（128M*4），达到四个时，HBase就会阻塞所有此时写入MemStore的请求，直到有一个MemeStore刷写完成，释放锁后，总的MemStore\u0026lt;4个，才能继续写入MemStore。 内存总和： HRegionServer 级别的刷写。整个 HRegionServer 的 各个HRegion中的各个Store中的MemStore 占用内存总和大于相关阈值时会触发刷写。此时当前 HRegionServer 的所有写操作将会被阻塞，这个阻塞可能会持续到分钟级别。 日志阈值：HBase 使用了 WAL（Write-Ahead Logging） 机制（日志先行），当数据到达 HRegion 时是先写入日志的，然后再被写入到 MemStore。如果日志的数量越来越大，这就意味着 MemStore 中未持久化到磁盘的数据越来越多（因为MemStore中的数据持久化到磁盘后，就会把对应的日志文件删除）。当 HRegionServer 挂掉的时候，恢复时间将会变得很长，所以有必要在日志到达一定的数量时进行一次刷写操作。 **定期刷写：**当定时器到达 hbase.regionserver.optionalcacheflushinterval （默认值 3600000 毫秒，即 1 小时）时，HBase 会自动触发刷写。一般建议调大，比如 10 小时，因为很多场景下 1 小时 Flush 一次会产生很多小文件，一方面导致Flush 比较频繁，另一方面导致小文件很多，影响随机读性能。 **更新频率：**如果 HBase 的某个 HRegion 更新的很频繁，而且既没有达到自动刷写阀值，也没有达到内存的使用限制，但是内存中的更新数量已经足够多，比如超过 hbase.regionserver.flush.per.changes 参数配置，默认为 30000000 次，也会触发刷写。 **手动刷写：**Shell客户端手动使用 flush 命令。 刷写策略： HBASE 1.1 之前：MemStore 刷写是 HRegion 级别的。就是说，如果要刷写某个 MemStore ，MemStore 所在的 HRegion中其他的 MemStore 也是会被一起刷写的（简单的理解：Flush 一个列族时其它列族也会一起 Flush）。 HBASE 2.x 之后：保留了HBASE 1.1 之前的刷写策略，还有其他的两个刷写策略，根据是否达到阈值计算公式的阈值，来进行刷写，如果这两个刷写策略，都不满足条件的话，就会退化成HBASE 1.1 之前的刷写策略。 刷写流程： prepareFlush 阶段：先对 MemStore 做 Snapshot快照，为了防止刷写过程中更新的数据同时在 Snapshot 和 MemStore 中而造成后续处理的困难，所以在刷写期间需要持有 updateLock。持有了 updateLock 之后，将阻塞客户端的写操作。 flushCache 阶段：将snapshot写入到临时文件中。将临时文件移动到对应的列族目录下，并做一些清理工作，如删除snapshot。 合并 合并是HBase将多个HFile文件合并成一个或多个更大的HFile文件的过程。这个过程可以减少HFile文件的数量，提高读性能，并清理过期或无效的数据。 HBase根据规模将合并分为了两类： Minor Compaction（次要/小） 选取一些小的、相邻的 StoreFile 将他们合并成一个更大的 StoreFile，在这个过程中不做任何删除数据、多版本数据的清理工作，但是会对 minVersion=0 并且设置 TTL 的过期版本数据进行清理。一次 Minor Compaction 的结果是让小的StoreFile 变的更少并且产生更大的 StoreFile。 Major Compaction（主要/大） 将所有的 StoreFile 合并成一个 StoreFile 清理三类无意义数据：被删除的数据、TTL 过期数据、版本号超过设定版本号的数据。 Scala # Scala 中，函数是一等公民具体体现在哪里 # 函数可以存储在变量中；函数可以作为参数；函数可以作为返回值。\n说说 Scala 函数的至简原则 # 方法和函数不建议写 return 关键字，Scala 会使用函数体的最后一行代码作为返回值； 方法的返回值类型如果能够推断出来，那么可以省略，如果有 return 则不能省略返回值类型，必须指定； 因为函数是对象，所以函数有类型，但函数类型可以省略，Scala 编译期可以自动推断类型； 如果方法明确声明了返回值为 Unit，那么即使方法体中有 return 关键字也不起作用； 如果方法的返回值类型为 Unit，可以省略等号 = ； 如果函数的参数类型能够推断出来，那么可以省略； 如果方法体或函数体只有一行代码，可以省略花括号 {} ； 如果方法无参，但是定义时声明了 () ，调用时小括号 () 可省可不省； 如果方法无参，但是定义时没有声明 () ，调用时必须省略小括号 () ； 如果不关心名称，只关心逻辑处理，那么函数名可以省略。也就是所谓的匿名函数； 如果匿名函数只有一个参数，小括号 () 和参数类型都可以省略，没有参数或参数超过一个的情况下不能省略 () ； 如果参数只出现一次，且方法体或函数体没有嵌套使用参数，则参数可以用下划线 _ 来替代。 Spark # 说 10 个常用的 Spark 转换算子 # 单Value\nmap 待处理数据逐条映射转换 mapPartitions \u0026amp; mapPartitionsWithIndex 易OOM 分区为单位进行数据转换 Index -能指定对哪个分区的数据~ flatMap 扁平化映射\u0026ndash; map+flatten 先数组 再展平为一个个的元素 集合 glom 同一个分区的所有单个数据转换成数组~ groupBy（shuffle） 分组 数据打乱重新组合=shuffle filter 过滤 sample 抽样 withReplacement fraction seed distinct（shuffle） 去重 coalesce（shuffle） 缩小扩大分区数 第二个参数是否shuffle repartition（shuffle） 重新分配分区数 底层调用coalesce 默认是shuffle的 sortBy（shuffle） 排序 默升 双 Value\nunion 两个 RDD 并集 不去重 intersection 交集 subtract cartesian 笛卡尔积 zip 拉链 分区数一致，分区类元素数一致 Key-Value\npartitionBy（shuffle） sortByKey（shuffle） reduceByKey（shuffle） 分区内和分区间的计算逻辑必须一致 groupByKey（shuffle） aggregateByKey（shuffle） 分区内和分区间的计算逻辑不一致 mapValues 只对 V 进行操作 foldByKey（shuffle） 分区内和分区间计算逻辑相同时 combineByKey（shuffle） join cogroup 两个RDD 先key分组\u0026ndash;再Value连接 说几个高效算子，并说明为什么高效 # mapPartitions：对比与map，一次处理一个分区的数据，而不是单个元素，意味着，每个分区函数只会调用一次，减少了函数调用的开销，类似于批处理，提高性能。 但是处理完的数据是不会被释放掉的，存在对象的引用，在内存较小，数据量较大的场合下，容易出现OOM。 ForeachPartiton：在Foreach中，每个元素都要执行一次指定的函数，而ForeachPartiton则是对一个分区执行一次指定的函数（该函数接收的参数是该分区的迭代器（Iterator）），减少了系统调用的次数，提高了效率。将RDD的数据批量写入数据库或者文件系统时，每个分区的数据只需要建立一次数据库连接，减少了数据库连接的开销。提高了性能。 reduceByKey和 aggregateByKey：这两个算子在shuffle操作之前对每个分区内的相同key进行预聚合，减少了网络传输的数据量，降低了磁盘IO，提高性能。可以缓解数据倾斜问题。 coalesce和repartition：合理的调整分区数量，使每个Task处理的数据量更加均衡。当后续的一些操作不需要高并行度时，可以减少分区数，来降低内存和CPU的开销。 persist和cache：将RDD的数据持久化到内存或磁盘中，便于后续操作中数据的重用（不需要每次都从头开始计算）。避免了重复计算相同数据集的开销，persist算子允许用户指定存储级别，如纯内存、内存加磁盘等，来适应不同的需求。 如果让你编码，如何实现 distinct 算子的功能 # 使用哈希表去重：由于哈希表不允许重复的键，‌因此，‌当尝试插入具有相同键的新元素时，‌哈希表会自动覆盖旧的值，‌从而实现去重的效果。‌\n排序后去重：遍历排序后的列表，并跳过重复的元素。\n如果需要处理大量数据，哈希表的效率更高；如果需要保证数据的有序性，排序算法更加适合。具体使用看不同的场景。\n1.将原始RDD转换成一个元组RDD，元组RDD的第一个元素是原始RDD的元素，第二个元素是null\n2.使用ReduceByKey算子对元组RDD进行分组，并统计每个分组中元素的个数\n3.使用filter算子过滤掉分组中元素个数大于1的元组\n4.将过滤后的元组RDD转换回原始RDD的类型\n说几个 spark-submit 提交应用的参数，并说明其作用 # \u0026ndash;master ：Spark 任务提交的模式，默认为本地模式。 local ：本地模式，只有一个工作进程，无并行计算能力； local[N] ：本地模式，有 N 个工作进程，通常设置 N 为机器的 CPU 核数； local[*] ：本地模式，工作进程数量等于机器的 CPU 核数。 Standalone 、Mesos、K8s、YARN模式 \u0026ndash;deploy-mode ：Driver 程序运行的地方，Client 或者 Cluster，默认是 Client； \u0026ndash;class ：要执行的程序的主类； \u0026ndash;jars ：逗号分隔的本地 JARS，Driver 和 Executor 依赖的第三方 Jar 包； \u0026ndash;name ：应用程序的名称； \u0026ndash;driver-memory ：Driver 程序使用内存大小（例如：1000M，5G），默认 1024M； \u0026ndash;executor-memory ：每个 Executor 内存大小（如：1000M，2G），默认 1G。 Hadoop MR 的 Shuffle 和 Spark Shuffle 的区别？？？？ # 数据处理方式： MR中Map和Reduce的中间结果，也就是shuffle需要将数据落地到磁盘，会有大量的磁盘I/O操作，从而影响性能。 Spark Shuffle ，Spark会尽量将中间结果持久化到内存中，以减少磁盘I/O操作。实在内存不够，才写入磁盘。基于内存，执行效率高。 优化方面： MR shuffle有大量的磁盘I/O，优化性能不是很好，优化空间有限。 Spark提供了多种Shuffle机制，如HashShuffle和SortShuffle，其中SortShuffle在数据量较大时能够提供更好的性能。 网络传输方式：Hadoop Shuffle 采用 HTTP 协议传输数据，而 Spark Shuffle 采用 NIO（非阻塞）传输协议。 Spark SQL 的执行流程 # Spark SQL的核心是Catalyst优化器：\n**解析：**SQL语句经过SqlParser解析成Unresolved LogicalPlan（未绑定的逻辑执行计划） **编译：**Analyzer（分析器）将Unresolved LogicalPlan 编译成 Analyzed Logical Plan（编译后的逻辑计划） **优化：**Logical optimizer（逻辑计划调优器）使用一些 Optimization Rules（调优规则：合并、列裁剪、谓词下推等）将 Analyzed Logical Plan 优化成 Optimized Logical Plan（优化后的逻辑计划） **物理执行计划：**Physical Planner（物理计划生成器）将Optimized Logical Plan转换成可执行的物理计划 Physical Plan **代码执行：**最后通过 Code Generation（代码生成器）把 SQL 生成底层的代码，最终执行。 讲讲 Spark 的通用运行流程 # Spark 的通用运行流程：\n启动集群后，Worker 节点会向 Master 节点心跳汇报资源（CPU Core 和 Memory）情况；\nClient 提交 Application，根据不同的运行模式在不同的位置创建 Driver 进程；\nSparkContext 连接到 Master，向 Master 注册应用并申请资源（Executor 的 CPU Core 和 Memory）；\nMaster 根据 SparkContext 的资源申请并根据 Worker 心跳周期内报告的信息决定在哪个 Worker 上分配资源，也就是\nExecutor；\nWorker 节点创建 Executor 进程，Executor 向 Driver 进行反向注册；\n资源满足后（Executor 注册完毕），SparkContext 解析 Applicaiton 代码，创建 RDD，构建 DAG，并提交给\nDAGScheduler 分解成 Stage（当碰到 Action 算子时，就会催生 Job，每个 Job 中含有 1 个或多个 Stage），然后将\nStage（或者称为 TaskSet）提交给 TaskScheduler，TaskScheduler 负责将 Task 分配到相应的 Worker，最后提交给\nExecutor 执行（发送到 Executor 的线程池中）；\n每个 Executor 会持有一个线程池，Executor 通过启动多个线程（Task）来对 RDD 的 Partition 进行并行计算，并向\nSparkContext 报告，直至 Task 完成。\n所有 Task 完成后，SparkContext 向 Master 注销，释放资源。\n任务执行失败的情况：\nTask 在 Executor 线程池中的运行情况会向 TaskScheduler 反馈，当 Task 执行失败时，会由 TaskScheduler 负责重试，\n将 Task 重新发送给 Executor 去执行，默认重试 3 次。如果重试 3 次依然失败，那么这个 Task 所在的 Stage 就失败了。\nStage 失败了会由 DAGScheduler 来负责重试，会重新发送 TaskSet 到 TaskScheduler，Stage 默认重试 4 次。如果重试 4\n次以后依然失败，那么这个 Job 就失败了。Job 失败则 Application 提交失败。\nTaskScheduler 不仅能重试失败的 Task，还会重试 Straggling（落后，缓慢）的 Task（也就是执行速度比其他 Task 慢太\n多的 Task）。如果有运行缓慢的 Task 那么 TaskScheduler 会启动一个新的 Task 来与这个运行缓慢的 Task 执行相同的\n处理逻辑。两个 Task 哪个先执行完，就以哪个 Task 的执行结果为准。这就是 Spark 的推测执行机制。在 Spark 中推\n测执行默认是关闭的。推测执行可以通过 spark.speculation 属性来配置。\n讲讲 Spark YARN Cluster 模式的运行流程 # 在 YARN-Cluster 模式中，当用户向 YARN 中提交一个应用程序后，YARN 会分两个阶段运行该应用程序：\n第一个阶段是把 Spark 的 Driver 作为一个 ApplicationMaster 在 YARN 集群中先启动； 第二个阶段是由 ApplicationMaster 创建应用程序，然后为它向ResourceManager 申请资源，并启动 Executor 来运行Task，同时监控它的整个运行过程，直到运行完成。 具体流程如下：\n在 YARN Cluster 模式下，Driver 运行在 ApplicationMaster 中。程序启动后会和 ResourceManager 通讯申请启动ApplicationMaster； ResourceManager 收到请求后，通过 ResourceScheduler 选择一台NodeManager 分配一个 Container，在 Container 中开启 ApplicationMaster 进程；同时在 ApplicationMaster 中初始化 Driver； ApplicationMaster 向 ResourceManager 注册，这样用户可以直接通过ResourceManage 查看应用程序的运行状态，然后它将采用轮询的方式通过 RPC 协议为各个任务申请资源，并监控它们的运行状态直到运行结束； 一旦 ApplicationMaster 申请到资源（也就是 Container）后，便与对应的 NodeManager 通信，在 NodeManager 的Container 中启动 CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend 启动后会向 Driver 中的 SparkContext反向注册并申请 Task。这一点和 Standalone 模式一样，只不过 SparkContext 在 Spark Application 中初始化时，使用CoarseGrainedSchedulerBackend 配合 YarnClusterScheduler 进行任务的调度，其中 YarnClusterScheduler 只是对TaskSchedulerImpl 的一个简单包装，增加了对 Executor 的等待逻辑等； ApplicationMaster 中的 SparkContext 分配 Task 给CoarseGrainedExecutorBackend 执行，CoarseGrainedExecutorBackend运行 Task 并向 ApplicationMaster 汇报运行的状态和进度，方便ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster 向 ResourceManager 申请注销并关闭自己。 该模式下只能通过 YARN 查看日志。 RDD，DataFrame，DataSet 的区别？？？？？ # RDD (Spark1.0) —\u0026gt; Dataframe(Spark1.3) —\u0026gt; Datase(Spark1.6)\nRDD里面的数据就只知道是一行干巴巴的数据，Spark框架并不知道这些数据的类型和含义。弱类型。\nDataFrame是以RDD为基础的分布式数据集。DataFrame 为数据提供了 Schema 视图，类似于传统数据库的二维表格，知道数据的类型和数据的含义，就是二维表的列、字段。支持Spark SQL查询和UDF函数。 弱类型。DataFrame = Dataset[Row]行类型\nDataSet是DataFrame的扩展，相当于里面的数据，对应java实体类一样，知道这行数据是哪个类的哪些属性。强类型。DataSet 支持编译时类型检查和优化，支持Spark SQL查询和UDF函数。\nSpark 的控制算子有哪些，有什么区别 # RDD是不会保存数据的，控制算子能把RDD数据进行持久化（缓存），便于数据重用。\ncache： 底层调用persist，且persist默认存储级别是存内存，把数据缓存在JVM的堆内存中。 会在血缘关系中添加新的依赖。一旦，出现问题，可以从头读取数据。 persist： 通过persist把存储级别设置为磁盘存储，数据保存到磁盘文件是临时文件，作业执行完会被删除。 涉及到磁盘IO，性能较低，但是数据安全。 checkpoint： 指定检查点保存路径（一般都是分布式存储系统，HDFS），数据长久的保存在磁盘文件中。可以跨作业执行。 涉及到磁盘IO，性能较低，但是数据安全。 会再创建一个作业，重新计算这个RDD的数据，再把把数据持久化到Checkpoint目录中。相当于走了两遍计算，这样效率更低了。 为了能够提高效率：一般情况下，需要和cache联合使用。checkpoint创建的的作业就不需要从头走一遍计算了，直接从cache的缓存中取数据然后持久化到磁盘文件 执行过程中，会切断血缘关系，重新建立新的血缘关系\u0026mdash;等同于改变了数据源。 这些持久化操作必须在遇到行动算子时才会执行，\nYARN 的 Client 提交和 Cluster 提交的区别 # Driver的运行位置： Client模式下Driver运行在客户端，也就是用户提交作业的服务器上运行Driver程序。 Cluster提交模式，Driver会在YARN集群中的一个NodeManager上运行。 容错性： Client端提交作业，如果Client端出问题，可能导致作业执行失败。 Cluster 提交，即使Client端关闭或失败，作业也能继续执行。 Spark 为什么比 MapReduce 快 # 内存：Spark基于内存，中间结果直接存放到内存中，MR基于磁盘，磁盘I/O开销大，速度慢。\n资源申请粒度：Spark粗粒度申请资源（Application 执行之前，一次性申请所有的资源），MR细粒度申请资源（每一个Task执行前去申请对应的资源）。\nRDD 中 reduceBykey 与 groupByKey 的区别 # **功能：**reduceBykey 先根据Key分组然后对其对应的Value进行聚合。groupByKey只根据Key进行分组，不能聚合。 **性能：**reduceByKey 在 Shuffle 前对分区内相同 Key 的数据进行预聚合（combine），减少了网络传输的数据量、落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。 Spark 数据本地化级别与区别 # Spark 数据本地化级别是 Spark 在执行作业时，根据数据和任务（Task）的相对位置来优化任务调度的一种方式。\n进程本地化： 指代码和数据在同一个进程中，也就是同一个 Executor 中； 性能最好，因为避免了网络传输开销。 这里的“同进程”更多地是关注于任务执行时代码和数据之间的接近性，以减少数据访问的延迟和开销。不同进程间的话要跨进程传输数据。 节点本地化： 数据和 Task 在同一个节点上，数据和 Task 在同一个节点不同的 Executor 中，数据需要跨进程传输； 无偏好： 对于 Task 来说，数据从哪里获取都一样，没有好坏之分。比如 SparkSQL 直接读取 MySQL； 机架本地化： 数据和 Task 在一个机架的两个节点上，数据需要通过网络在节点之间进行传输； 任意： 数据和 Task 可能在集群中的任何地方，而且不在一个机架中，性能最差。 Spark 单表 SQL 分组聚合查询数据倾斜如何解决 ？？？ # 增加Shuffle Partitions数量 在Spark中，可以通过增加Shuffle Partitions的数量来减小数据倾斜。Shuffle Partitions是Spark在执行Shuffle操作时使用的分区数，增加这个数量可以使得数据更加均匀地分布在不同的分区中，从而减小单个分区的数据量，提高并行度。 解决逻辑：两阶段聚合（加盐局部聚合 + 去盐全局聚合）+ Map-Side 聚合（将 groupByKey 算子转换为 reduceByKey 算子，reduceByKey 可以在 Map 端预聚合，类似于 MapReduce 中的 Combiner）。 首先，通过 map 算子给每个数据的 Key 添加随机数前缀，对 Key 进行打散，将原先一样的 Key 变成不一样的 Key，然 后进行第一次聚合，这样就可以让原本被一个 Task 处理的数据分散到多个 Task 上去做局部聚合；随后，去除掉每个 Key 的前缀，再次进行聚合 Spark JOIN SQL 查询数据倾斜如何解决 ？？？ # 小表join大表，广播join，将小表的数据分发到每个节点上，供大表使用 大表join小表，大表加严将key打散，小表key扩容，join完，union all\n广播JOIN：适用于小表 JOIN 大表。小表足够小，可被加载到 Driver 端并通过 Broadcast 广播到各个 Executor 中。\n在小表 JOIN 大表时如果产生数据倾斜，那么广播 JOIN 可以直接规避掉 Shuffle 阶段（默认开启）。\n查看 SQL 执行计划的关键字是什么，有什么用 # explain\n从 3.0 开始，explain 提供了一个新的参数 mode，通过该参数可以指定执行计划的展示格式：\nexplain(mode=\u0026ldquo;simple\u0026rdquo;) ：只展示物理执行计划； explain(mode=\u0026ldquo;extended\u0026rdquo;) ：展示物理执行计划和逻辑执行计划； explain(mode=\u0026ldquo;codegen\u0026rdquo;) ：展示 Code Generation 生成的可执行 Java 代码； explain(mode=\u0026ldquo;cost\u0026rdquo;) ：展示优化后的逻辑执行计划以及相关的统计； explain(mode=\u0026ldquo;formatted\u0026rdquo;) ：以分隔的方式输出，输出更易读的物理执行计划，并展示每个节点的详细信息。 通过explain能看到查询是如何执行的、查询的各个详细的步骤，包括查询的顺序、连接方法、使用的索引等。以便进行针对性的优化。\n说一说 Spark 你知道的优化（问的几率不大，时间充足可以看看） # CPU优化： 内存优化： SQL优化： RBO优化 CBO优化 JOIN优化 数据倾斜优化 Job优化 Shuffle 优化 代码优化 SQL # abc.txt 按空格分割 WordCount SQL 实现思路 # 1 创建 DataFrame: 准备包含文本数据的 DataFrame，假设有一个列名为 text，其中包含要分析的文本数据。 2 按空格分割单词: 使用 split 函数按空格将文本分割为单词数组。 3 展开数组: 使用 explode 函数将数组中的单词展开为新的行。 4 分组和计数: 使用 groupBy 和 count 函数对单词进行分组并计数。 5 排序结果 (可选): 使用 orderBy 对计数结果进行排序，以便查看最常出现的单词 #子查询 SELECT word,count(*) cnt FROM( SELECT EXPLODE(split(REGEXP_REPLACE(line,\u0026#34;\\[^\\\\w\u0026#39;-]+\u0026#34;,\u0026#34; \u0026#34;),\u0026#34;\\\\s+\u0026#34;)) AS word FROM t_wordcount) t GROUP BY word ORDER BY cnt desc; #侧视图 SELECT word,count(*) cnt FROM t_wordcount LATERAL VIEW EXPLODE(split(REGEXP_REPLACE(line,\u0026#34;\\[^\\\\w\u0026#39;-]+\u0026#34;,\u0026#34; \u0026#34;),\u0026#34;\\\\s+\u0026#34;)) lv AS word GROUP BY word ORDER BY cnt desc; 求部门前三薪资 SQL 实现思路 # -- 不开窗 -- 一个部门里面，如果其它人的工资只有0个、1个、2个人的比我高（也就是工资比我高的人少于3个），那么我就是前三名的 select * from emp e1 where (select count(*) from emp e2 where e1.deptno = e2.deptno and e2.sal \u0026gt; e1.sal ) \u0026lt;3 order by deptno,sal desc -- 开窗 -- 查询各个部门的员工工资 按sal降序排 设置序号 select *,row_number() over(partition by deptno order by sal desc) r from emp -- 取前三个 select * from (select *,row_number() over(partition by deptno order by sal desc) r from emp) t where t.r\u0026lt;=3 连续 7 天登录 SQL 实现过程 # -- 开窗 -- 窗口分组 时间排序 去除同一天不同时间段的日期 后面加个序号 -- 把登陆日期减去右边的排序，如果是连续的话，减去排序后的日期是一样的，按user_id和减去排序后的日期分组，统计数量，数量\u0026gt;=7 就是连续登录7天的 select user_id,count(*) cnt from ( select *,row_number() over(partition by user_id order by sd) r from ( select distinct user_id,date_format(sign_date,\u0026#39;%Y%m%d\u0026#39;) sd from user_sign order by user_id ) t ) tt group by user_id,DATE_SUB(sd, INTERVAL r DAY) having cnt \u0026gt;= 7 统计每个员工前三个月的薪水，每月统计一次 # -- 滑动窗口range between -- 6. （困难）力扣 579题，查询员工的累计薪水（统计每个员工前三个月的薪水，每月统计一次）。 -- 按user_id分组 month排序 sum(salary)聚合函数 + 滑动窗口 当前行开始加上前面两行的salary select user_id,month,salary, sum(salary) over(partition by user_id order by month RANGE BETWEEN 2 PRECEDING AND CURRENT ROW ) from leetcode_579 好友推荐 SQL 实现思路 # WITH df AS(\r-- 创建临时表 df，包含用户及其朋友，初始亲密度设为 0\rSELECT login_user,friend,0 intimacy FROM t_friend\r-- 使用 LATERAL VIEW EXPLODE 将 friends 列展开为多行\rLATERAL VIEW EXPLODE(friends)lv AS friend\r-- 只选择 login_user 大于 friend 的记录，以避免重复的朋友关系\rWHERE login_user\u0026gt;friend\r),\rtemp AS(\r-- 创建另一个临时表 temp，包含所有用户及其朋友\rSELECT login_user,friend FROM t_friend\r-- 同样使用 LATERAL VIEW EXPLODE 展开 friends 列\rLATERAL VIEW EXPLODE(friends)lv AS friend\r),\ridf AS(\r-- 创建临时表 idf，计算每对朋友间的亲密度，初始设为 1\rSELECT t1.friend AS login_user,t2.friend, 1 intimacy\r-- 自连接 temp 表，以获得同一用户的所有朋友配对\rFROM temp t1 join temp t2 ON t1.login_user =t2.login_user\r-- 只选择 t1.friend 大于 t2.friend 的记录，避免重复配对\rWHERE t1.friend\u0026gt;t2.friend\r),\rrecommend AS(\r-- 合并 df 和 idf 表中的数据\rSELECT login_user,friend,intimacy FROM df UNION ALL\rSELECT login_user,friend,intimacy FROM idf )\r-- 从合并后的表中选择结果\rSELECT login_user,friend,sum(intimacy) si FROM recommend\r-- 按 login_user 和 friend 分组，计算每对的亲密度总和\rGROUP BY login_user,friend -- 筛选出亲密度总和等于记录数量的结果\rHAVING sum(intimacy)=count(intimacy)\r-- 按 login_user 和亲密度总和降序排序\rORDER BY login_user,si DESC; ","date":"2024-07-01","externalUrl":null,"permalink":"/docs/shell/","section":"Docs","summary":"第一阶段面试题 # Linux # 说 10 个常用的 Linux 命令 # 重启网络：systemctl restart network","title":"BigData interview1","type":"docs"},{"content":" HBase # HBase是什么？ # 简介 # ​\t是一个开源的、高可靠性、高性能、面向列（这里指列族，非列式存储）、可伸缩、实时读写的分布式数据库，其设计思想来源于 Google 的 BigTable 论文。利用 Hadoop HDFS 作为其文件存储系统，利用 ZooKeeper 作为其分布式协同服务。主要用来存储非结构化和半结构化的松散数据（列式存储 NoSQL 数据库）。总而言之：HBase是一种基于HDFS的分布式、面向列的非关系型数据库。\n特点 # 易扩展：\nHBase 的扩展性主要体现在两个方面，一个是基于运算能力（HRegionServer） 的扩展，通过增加 HRegionSever 节点 的数量，提升 HBase 上层的处理能力；另一个是基于存储能力的扩展（HDFS），通过增加 DataNode 节点数量对存储层的 进行扩容，提升 HBase 的数据存储能力。\n容量大：\nHBase 单表可以有十亿行、百万列，数据矩阵横向和纵向两个维度所支持的数据量级都非常具有弹性。HBase 的主要 作用就是面向 PB 级别数据的实时入库和快速随机访问。这主要源于上述易扩展的特点，使得 HBase 通过扩展来存储海量 的数据。\n面向列：\nHBase 是根据列族来存储数据的，列族下面可以有非常多的列。列式存储的最大好处就是，其数据在表中是按照某列 存储的，这样在查询只需要少数几个字段时，能大大减少读取的数据量。还可以动态增加列，可单独对列进行各方面的操 作。\n多版本：\nHBase 的每个列的数据存储支持多个 Version，比如住址列，可能有多个变更。\n稀疏性：\n为空的列并不占用存储空间，表可以设计的非常稀疏。不必像关系型数据库那样需要预先知道所有列名然后再进行 null 填充。\n高可靠：\nWAL（Write Ahead Log）日志先行机制，保证数据写入的时候不会因为集群异常而导致写入数据丢失。Replication 机 制，保证了集群在出现严重问题的时候，数据不会发生丢失或者损坏。HBase 底层使用 HDFS，本身也有备份。\n高性能：\n底层的LSM（Log-Structured Merge-tree）数据结构和Rowkey有序排列等设计，使得HBase具备非常高的写入性能，同时对于高并发的场景也具备很好的适应能力。\n应用场景？ # HBase 是一种 NoSQL（非关系型） 数据库，这意味着它不像传统的 RDBMS（关系型） 数据库那样支持 SQL 作为查询语言。HBase 是一种分布式存储的数据库，技术上来讲，它更像是分布式存储而不是分布式数据库，它缺少很多 RDBMS 系统的特性，比如列类型，辅助索引，触发器和高级查询语言等。\nHBase适合数据量要特别大，如果有十亿或百亿行数据，那么 HBase 是一个很好的选择，如果只有几百万行甚至不到的数据量，RDBMS 是一个很好的选择。因为数据量小的话，真正能工作的机器量少，剩余的机器都处于空闲的状态。其次，如果你不需要辅助索引，静态类型的列，事务等特性可以考虑 HBase。但是一个已经用 RDBMS的系统想要切换到 HBase，则需要重新设计系统。最后，保证硬件资源足够，每个 HDFS 集群在少于 5 个节点的时候，都不能表现的很好。因为 HDFS 默认的复制数量是 3，再加上一个 NameNode。其实HBase 在单机环境下也能运行，但是请在开发环境中进行使用。\n适合 HBase 的应用：\n存储业务数据：车辆 GPS 信息，司机点位信息，用户操作信息，设备访问信息。 存储日志数据：架构监控数据（登录日志，中间件访问日志，推送日志，短信邮件发送记录），业务操作日志信息。 存储业务附件：UDFS 系统（去中心化文件系统）存储图像，视频，文档等附件信息。 HBase 和 RDBMS 的区别：\n属性 HBase RDBMS 数据类型 只有字符串 丰富的数据类型 数据操作 增删改查，不支持 JOIN 各种各样的函数与表连接 存储模式 列族式存储 表结构和行式存储 数据保护 更新后仍然保留旧版本 替换 可伸缩性 轻易增加节点 需要中间层，牺牲性能 数据模型 # 在 HBase 表中，一条数据拥有一个全局唯一的主键（RowKey）和任意数量的列（Column Qualifier），每个列的数据 存储支持多个版本（Version），一列或多列组成一个列族（Column Family），同一个列族中列的数据在物理上都存储在 同一个 HFile 中。这样基于列存储的数据结构有利于数据缓存和查询。 所以，在 HBase 中定位一条数据需要通过：RowKey → Column Family → Column Qualifier → Version。\nHBase 表中的数据是疏松地存储的，因此用户可以动态地为数据定义各种不同的列。HBase 中的数据按主键排序（字 典序），同时 HBase 会将表按主键划分为多个 HRegion 存储在不同的 HRegionServer 上，以完成数据的分布式存储和读 取。\nNameSpace # 命名空间类似于关系型数据库中的数据库的概念，相当于表在逻辑上的分组（不同表放不同库中），命名空间是可以管理维护的，可以创建，删除或更改命名空间。HBase 有两个特殊预定义的命名空间：\ndefault：没有明确指定命名空间的表将默认在此命名空间下 hbase：系统命名空间，用于包含 HBase 的内部表（namespace）和元数据表(meta)（存了整个HBase中的所有其他的表的【数据与region，以及region与RegionServer的映射】）\u0026ndash;通过该机制能快速定位到数据。提高读写效率。 Table # 这里的Table和关系型数据库中的表差不多，行列的结构组成。\nRowKey\u0026ndash;行健 # RowKey类似于RDBMS中的主键，是一行数据的唯一标识。RowKey 可以是任意字符串（最大长度是64KB，实际应用中长度一般为 10-100 Bytes），RowKey 以字节数组保存。存储数据时，数据会按照 RowKey 的字典序排序存储，所以设计 RowKey 时，要充分利用排序存储这个特性，将经常一起读取的行存放到一起。\n字典排序：它基于字符的编码值（如ASCII码值或Unicode码值）进行排序。字符串被视为由若干个字符组成的序列，排序结果取决于字符之间的比较顺序。\n排序原理：\n字符串的比较是从左到右逐个字符进行的。 首先比较第一个字符，如果相同，则继续比较第二个字符，以此类推，直到遇到不同的字符或某个字符串结束。 如果某个字符串是另一个字符串的前缀，则较短的字符串排在前面。 如果两个字符串的所有字符都相同，则它们在排序中的位置由实现细节决定，但通常认为它们是等价的。 Column Family # Column Family 即列族，HBase 基于列划分数据的物理存储，同一个列族中列的数据在物理上都存储在同一个 HFile 中。一个列族可以包含任意多列，一般同一类的列会放在一个列族中。\nHBase 在创建表的时候就必须指定列族。HBase 的列族不是越多越好，官方推荐一个表的列族数量最好\u0026lt;=3，过多的列族不利于 HBase 数据的管理和索引。\nColumn Qualifier # 列族下的列，列标识，列标识是可以改变的，因此每一行可能有不同的列标识。使用的时候必须 列族:列 ，列可以根据需求动态添加或者删除。\nTimestamp # Timestamp 是实现 HBase 多版本的关键。在 HBase 中，使用不同的 Timestamp 来标识相同 RowKey 对应的不同版本 的数据。相同 RowKey 的数据按照 Timestamp 倒序排列，默认查询的是最新的版本，当然用户也可以指定 Timestamp 的值 来读取指定版本的数据。\nHBase 通过 RowKey 和 Column Family，Column Qualifier 来确定一个存贮单元（cell单元格），然后再通过时间戳来进行索引，找到对应的数据。为了避免数据存在过多版本而造成管理（包括存贮和索引）负担，HBase 提供了两种数据版本回收方案：\n一是保存数据的最后 n 个版本 二是保存最近一段时间内的版本（比如最近七天） Cell # Cell 由 Row，Column Family，Column Qualifier，Version 组成\u0026ndash;最后定位到的那个单元格。Cell 中的数据是没有类型的，全部使用字节码形式存贮，因为 HDFS 上的数据都是字节数组。HDFS中，虽然文件在物理上被切分成了多个块Block，但从逻辑上看，每个块仍然是由字节数组组成的。这意味着，无论数据在HDFS中如何被切分和存储，其本质仍然是字节序列。\n架构模型 # ​\tHBase 可以将数据存储在本地文件系统，也可以存储在 HDFS 文件系统。在生产环境中，HBase 一般运行在 HDFS 上，以 HDFS 作为基础的存储设施。用户通过 HBase Client 提供的 Shell 或 Java API 来访问 HBase 数据库，以完成数据的 写入和读取。HBase 集群主要由 HMaster、HRegionServer 和 ZooKeeper 组成。\nClient # HBase Client 为用户提供了访问 HBase 的接口，可以通过元数据表（客户端负责发送请求到数据库）来定位到目标数 据的 HRegionServer。客户端连接的方式有很多种：比如 HBase Shell 和 Java API。\n这里说的客户端请求指的是DDL、DML、DQL这些跟SQL语言差不多的操作，这里主要用的是HBase提供的相关操作的命令。\nZooKeeper # HBase 通过 ZooKeeper 来完成选举 HMaster、监控 HRegionServer、元数据管理等工作。主要工作职责如下：\n选举 HMaster：保证任何时候，集群中只有一个 HMaster处于Active状态（HMaster Active），其余处于备用状态（HMaster Backup）。 监控 HRegionServer（节点探活）：实时监控 HRegionServer 的状态，将 HRegionServer 的上下线信息实时报告给HMaster； 元数据管理维护：存储了HBase中的所有元数据信息（hbase:meta 元数据表\u0026mdash;-HRegion的寻址入口），存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 行键、列族、列限定符和时间戳、单元格cell。 HMaster # HMaster 是 HBase 整个集群的主节点，负责整个集群的管理工作，HMaster 可以实现高可用（Active 和 Backup），通过 ZooKeeper 来维护主备节点的切换。\nHMaster的主要作用：\n管理分配：HMaster负责管理RegionServer的注册和心跳，监控RegionServer的状态。管理和分配 HRegion，启动的时候分配 HRegion 到具体的 HRegionServer。在RegionServer出现故障时，HMaster会重新分配Region，确保数据的可用性和服务的连续性。 负载均衡：一方面负责将用户的数据均衡地分布在各个 HRegionServer 上，防止 某个HRegionServer 数据倾斜过载。另一 方面负责将用户的请求均衡地分布在各个 HRegionServer 上，防止 HRegionServer 请求过热； **表管理：**HMaster负责创建、删除表以及进行表的修改操作（是把命令发送给RegionServer，让它去干活）。将表的定义信息同步到所有RegionServer上。 **维护数据：**发现失效的 HRegion，并将失效的 HRegion 分配到正常的 HRegionServer 上。当某个 HRegionServer 下线 时迁移其内部的 HRegion 到其他 HRegionServer 上。 处理客户端请求：客户端请求先通过zookeeper获取到meta元数据表的信息。再通过HMaster节点，HMaster会根据请求的表和RowKey确定请求的Region所在的RegionServer，并将请求转发给对应的RegionServer进行处理。 权限控制：当用户尝试执行涉及表结构或集群管理的操作时，HMaster 会验证用户的权限。HMaster 可以接收来自管理员的权限变更请求，并更新相应的权限信息。当用户尝试执行创建表、删除表、修改表结构等操作时，HMaster 会检查用户的权限。 HRegionServer # HRegionServer 直接对接用户的读写请求，是真正干活的节点，属于 HBase 具体数据的管理者。\n主要作用：\n实时和 HMaster 保持心跳，汇报当前节点的信息； 当接收到 HMaster 的命令创建表时，会分配一个 HRegion 对应一张表； 负责切分在运行过程中变得过大的 HRegion； 当 HRegionServer 意外关闭的时候，当前节点的 HRegion 会被其他 HRegionServer 管理； 维护 HMaster 分配给它的 HRegion，处理对这些 HRegion 的 IO 请求； 当客户端发送 DML 和 DQL 操作时，HRegionServer 负责和客户端建立连接； WAL：Write Ahead Log 日志先行。记录了数据写入、更新日志，它被用来做故障恢复；会先把数据写入到HLog中。 MemStore：【写缓存】，数据首先会被写入到 MemStore 中（在写入日志之后）。每个 HRegion 的每个 Column Family 都会有一个MemStore。 负责与底层的 HDFS 交互，存储数据（HLog、HFile）到 HDFS。 BlockCache：【读缓存】，在内存中存储了最常访问的数据，采用 LRU 机制进行淘汰。\u0026mdash;\u0026ndash;具体见读流程。 当某个 HRegionServer 宕机后，zookeeper会通知HMaster进行失效备援。下线的 HRegionServer 所负责的 HRegion 暂 时停止对外提供服务，HMaster 会将该 HRegionServer 所负责的 HRegion 转移到其他 HRegionServer 上，并且会对下线的 HRegionServer 进行日志重放，将 MemStore 中还未持久化到磁盘中的数据进行恢复。\n当某台 HRegionServer Failover（故障转移） 的时候，整个过程中 HRegion 中的数据是不可用的，因为它是缺失的。因此，HBase 属 于 CP 架构，降低了可用性，具备强一致性读/写。设想一下，如果 Redo （恢复）过程中的 HRegion 能够响应请求，那么可用性提 高了，则必然返回不一致的数据（因为 Redo 可能还没完成），那么 HBase 的一致性就降低了。\nHRegion # 一个 HRegionServer 上包含了多个 HRegion，一个 HRegion 由多个 Store 组成，每个 Store 都对应一个 Column Family，Store 包含 1 个 MemStore 和 0 或多个StoreFile 组成。\nHBase 将表中的数据基于 RowKey 的不同范围划分到不同 HRegion 上，每个HRegion 都负责一定范围的数据存储和访问。当HBase表被创建时，它通常会被初始化为一个HRegion，包含整个行键空间。随着数据的不断插入，HRegion的大小会逐渐增长。HRegionServer会定期检查HRegion的大小，发现其大小超过了预设的阈值（10G==所有的StoreFile文件大小）时，触发分区操作按一定的规则进行分区。为了实现负载均衡，有可能新的分区会被分到其他的 HRegionServer 上。\n为了防止前期数据的处理都集中在一个 HRegionServer，我们可以根据自己的业务进行预分区。\n这样即使有一张百亿条数据的表，由于数据被划分到不同的 HRegion上，每个 HRegion 都可以独立地进行读写，HBase 读写数据的时候还可以与多 HRegion 分布式并发操作，所以访问速度并不会有太大的降低。\n负载均衡：防止一个RegionServer的压力过大，把Region分散到不同的RegionServer上\u0026ndash;HMaster负责。 Split （分Region）：在 HBase 中 Split 是一个很重要的功能，HBase 是通过把数据分配到一定数量的 HRegion 来达到负载均衡的。一个Table 会被分配到一个或多个 HRegion 中，这些 HRegion 会被分配到一个或者多个 HRegionServer 中。在自动 Split 策略 中，当一个 HRegion 达到一定的大小就会自动 Split 成两个 HRegion。Table 在 HRegion 中是按照 RowKey 来排序的，并且 一个 RowKey 所对应的行只会存储在一个 HRegion 中，这一点保证了 HBase 的强一致性。 当一个 Table 刚被创建的时候，HBase 默认的分配一个 HRegion 给 Table。也就是说这个时候，所有的读写请求都会访 问到同一个 HRegionServer 的同一个 HRegion 中，这个时候就达不到负载均衡的效果了，集群中的其他 HRegionServer 可 能处于比较空闲的状态。解决这个问题可以用 pre-splitting 在创建 Table 时提前生成多个 HRegion。 在 Table 初始化的时候如果不配置的话，HBase 是不知道如何去 Split HRegion 的，因为 HBase 不知道应该把哪个 RowKey 作为 Split 的开始点。如果我们可以大概预测到 RowKey 的分布，我们可以使用 pre-spliting 来帮助我们提前 Split HRegion。 如果我们的预测不是特别准确，还是会导致某个 HRegion 过热被集中访问，不过还好我们还有 auto-split，默认按 10G 自动切分。但是如果文件到达 9G 后迟迟未到 10G 此时对于 HBase 来说是比较难受的。最好的办法就是首先预测 Split 的切 分点，做 pre-splitting，后面再交给 auto-split 来处理。 HBase 在每次数据合并之后都会针对相应 HRegion 生成一个 requestSplit 请求，requestSplit 首先会执行 checkSplit， 检测 FileSize 是否达到阈值，如果超过阈值，就进行切分。 HBase 自带了两种 pre-split 的算法，分别是 HexStringSplit 和 UniformSplit 。如果我们的 RowKey 是十六进制 的字符串作为前缀的，就比较适合用 HexStringSplit 作为 pre-split 的算法。例如，我们使用 HexHash(prefix) 作为 RowKey 的前缀，其中 HexHash 为得到十六进制字符串的 hash 算法。我们也可以用我们自己的 Split 算法。 当一个 HRegion 达到一定的大小，他会自动 Split 成两个 HRegion。如果我们的 HBase 版本是 0.94 之后，那么默认的 有三种自动 Split 的策略，ConstantSizeRegionSplitPolicy，IncreasingToUpperBoundRegionSplitPolicy 还有 KeyPrefixRegionSplitPolicy。 在 0.94 版本之前 ConstantSizeRegionSplitPolicy 是默认和唯一的 Split 策略。当某个 Store（对应一个 Column Family） 的大小大于配置值 hbase.hregion.max.filesize 的时候（默认 10G）HRegion 就会自动分裂。 而 0.94 版本之后 IncreasingToUpperBoundRegionSplitPolicy 是默认的 Split 策略。这个策略中，最小的分裂大小和 Table 的某个 HRegionServer 的 HRegion 个数有关，当 StoreFile 的大小大于以下公式得出的值的时候就会 Split。公式如 下： # R 为同一个 Table 中在同一个 HRegionServer 中的 HRegion 的个数 Min(R^2 * \u0026#34;hbase.hregion.memstore.flush.size\u0026#34;, \u0026#34;hbase.hregion.max.filesize\u0026#34;) 例如：\nhbase.hregion.memstore.flush.size 默认值 128MB。 hbase.hregion.max.filesize 默认值为 10GB。 如果初始时 R=1 ，那么 Min(128MB, 10GB)=128MB ，也就是说在第一个 Flush 的时候就会触发分裂操作。 当 R=2 的时候 Min(22128MB, 10GB)=512MB ，当某个 StoreFile 大小达到 512MB 的时候，就会触发分裂。 如此类推，当 R=9 的时候，StoreFile 达到 10GB 的时候就会分裂，也就是说当 R\u0026gt;=9 的时候，StoreFile 达到 10GB 的时候就会分裂。 ​\tKeyPrefixRegionSplitPolicy 可以保证相同的前缀的 RowKey 保存在同一个 HRegion 中。指定 RowKey 前缀位数划分HRegion，通过读取 KeyPrefixRegionSplitPolicy.prefix_length 属性，该属性为数字类型，表示前缀长度，在进行 Split 时，按此长度对 SplitPoint 进行截取。此种策略比较适合固定前缀的 RowKey。当 Table 中没有设置该属性，指定此策略效果等同与使用IncreasingToUpperBoundRegionSplitPolicy。 我们可以通过配置 hbase.regionserver.region.split.policy 来指定 Split 策略，也可以写我们自己的 Split 策略。\nStore # 一个 HRegion 由多个 Store 组成，每个 Store 都对应一个 Column Family，Store 包含 1 个 MemStore 和 0 或多个 StoreFile 组成。\nMemStore：作为 HBase 的内存数据存储，数据的写操作会先写到 MemStore 中，当 MemStore 中的数据增长到指定阈 值（默认 128M）后，HRegionServer 会启动 FlushCache 进程将 MemStore 中的数据写入 StoreFile 持久化存储(刷写)，每次写 入后都形成一个单独的 StoreFile。当客户端检索数据时，先在 MemStore 中查找，如果 MemStore 中不存在，则会在 StoreFile 中继续查找。 StoreFile：保存实际数据的物理文件，它是HBase数据存储系统的基本单位，用于在HDFS（Hadoop Distributed File System）上存储数据。HBase 以 StoreFile 的大小来判断是否需要切分 HRegion。当一个 HRegion 中所有 StoreFile 的大小和数量都增长到超过指定阈值时，HMaster会把当前 HRegion 分割Split为两个，切分后其中一个 HRegion 会被转移到其他的 HRegionServer 上，实现负载均衡。 HFile：HFile 和 StoreFile 是同一个文件，只不过站在 HDFS 的角度称这个文件为 HFile，站在 HBase 的角度就称这个文件为StoreFile。是 HBase 在 HDFS 中存储数据的格式，它包含多层的索引，这样在 HBase 检索数据的时候就不用完全的加载整个文件。 StoreFile是HBase中保存数据的物理文件，而HFile是这些物理文件的底层实现格式。 HLog # 一个 HRegionServer 只有一个 HLog 文件。负责记录数据的操作日志，当 HBase 出现故障时可以进行日志重放、故障恢复。例如磁盘掉电导致 MemStore 中的数据没有持久化存储到 StoreFile，这时就可以通过 HLog 日志重放来恢复数据。\n在数据由（Write-Ahead Logging，预写日志）机制写入HLog后，数据还会被写到MemStore中。HLog文件会持久化存储到HDFS中。\nHDFS # HDFS 为 HBase 提供底层数据存储服务，同时为 HBase 提供高可用支持。HBase 将 HLog 存储在 HDFS 上，当服务器发 生异常宕机时，可以日志重放 HLog 来恢复数据。\n","date":"2024-07-01","externalUrl":null,"permalink":"/docs/hbase/","section":"Docs","summary":"HBase # HBase是什么？ # 简介 # ​\t是一个开源的、高可靠性、高性能、面向列（这里指列族，非列式存储）、可伸缩、实时读写的分布式数据库，其设计思想来源于 Google 的 BigTable 论文。利用 Hadoop HDFS 作为其文件存储系统，利用 ZooKeeper 作为其分布式协同服务。主要用来存储非结构化和半结构化的松散数据（列式存储 NoSQL 数据库）。总而言之：HBase是一种基于HDFS的分布式、面向列的非关系型数据库。","title":"HBase","type":"docs"},{"content":"\rShell 编程概述 # Shell 本身并不是内核的一部分，它只是站在内核的基础上编写的一个应用程序，它和 QQ、迅雷、Firefox 等其它软件没有什么区别。然而 Shell 也有着它的特殊性，就是开机立马启动，并呈现在用户面前；用户通过 Shell 来使用 Linux，不启动 Shell 的话，用户就没办法使用 Linux。\n在计算机科学中，Shell 俗称壳（用来区别于核），是指“为使用者提供操作界面”的软件（command interpreter，命令解析器）。它类似于 DOS 下的 COMMAND.COM 和后来的 cmd.exe。它接收用户命令，然后调用相应的应用程序。\nShell 并不是简单的堆砌命令，我们还可以在 Shell 中编程，这和使用 C++、Java、Python 等常见的编程语言并没有什么两样。\nShell 虽然没有 C++、Java、Python 等强大，但也支持了基本的编程元素，例如：\n变量、数组、字符串、注释、加减乘除、逻辑运算等概念； if\u0026hellip;else 选择结构，case\u0026hellip;in 开关语句，for、while、until 循环； 函数，包括用户自定义的函数和内置函数（例如 printf、export、eval 等）。 站在这个角度讲，Shell 也是一种编程语言，它的编译器（解释器）是 Shell 这个程序。我们平时所说的 Shell，有时候是指连接用户和内核的这个程序，有时候也指 Shell 编程。\nShell 名词解释 # Shell # Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 也是一个命令行解释器，是用户和内核之间的接口。用户可以在 Shell 中输入命令，然后，它解释命令来执行所需的任务。此外，它还可以执行程序和 Shell 脚本。Shell 脚本是一组命令，用户应该遵循标准语法向 Shell 写入命令。\n总的来说，Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。Shell 既是一种命令语言，又是一种程序设计语言，如果要与内核打交道就必须学习 Shell 语言。\n分类 # Shell 是提供与内核沟通接口的命令解释器程序，但实际上 Shell 是这种解释器的统称，Linux 系统的 Shell 种类很多，包括 Bourne Shell（简称 sh）、Bourne Again Shell（简称 bash）、C Shell（简称 csh）、K shell（简称 ksh）、Shell for Root 等等。如下图：\n也就是说 sh 和 bash 都是 Linux 系统 Shell 的一种，其中 bash 命令是 sh 命令的超集，大多数 sh 脚本都可以在 bash 下运行。Linux 系统中预设默认使用的就是 bash。\n要想知道操作系统支持哪种Shell类型，可在终端中输入以下命令：\n[root@node01 ~]# cat /etc/shells /bin/sh /bin/bash /usr/bin/sh /usr/bin/bash 要想知道 bash 在操作系统中具体的位置，可通过以下命令查看：\n[root@node01 ~]# which bash sh /usr/bin/bash /usr/bin/sh She Bang # She Bang 是 Shell 脚本开头字符#!也可以叫 Sha Bang，当 Shell 文件被 Linux 系统读取时，内核会通过#!表示的值（0x23, 0x21）识别出随后的解释器路径并调用，最后再将整个脚本内容传递给解释器。由于 Shell 当中#字符同时表示注释，因此 Shell 解释脚本文件时会自动忽略该行。\n总结：#!就是告诉系统解释此脚本文件的 Shell 程序在哪（其后路径所指定的程序）。例如：\n#!/bin/bash echo \u0026#34;Hello World!\u0026#34; She Bang 的格式很重要，格式不正确会导致命令工作不正常。因此，在创建脚本时，要始终记住 She Bang 格式的这两点：\n它应该始终在脚本的第一行。 在#!和解释器的路径之间，#之前不应有任何空格。 echo是 bash 中的内置命令，用于通过传递参数来显示标准输出。它是用于将文本/字符串行打印到屏幕上的最广泛使用的命令。\n脚本 # 在计算机编程中，脚本是用于适当的运行时环境的一组命令，这些命令用于自动执行任务。\n我们经常说的 Shell 脚本，其实就是利用 Shell 的功能，编写能够直接运行的脚本文件。\n第一个 Shell 脚本 # 几乎所有的编程语言教程都是从著名的“Hello World”开始，出于对这种传统的尊重，我们的第一个 Shell 脚本也输出“Hello World”。\n打开文本编辑器，新建一个文本文件，并命名为 hello.sh。\n扩展名sh代表 shell，扩展名并不影响脚本执行，见名知意就好。\n在 hello.sh 中输入代码：\n#!/bin/bash echo \u0026#34;Hello World!\u0026#34; #!是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell；后面的/bin/bash就是指明了解释器的具体位置。\necho命令用于向标准输出文件（Standard Output，stdout，一般就是指显示器）输出文本。在.sh文件中使用命令与在终端直接输入命令的效果是一样的。\n接下来使用bash或sh运行脚本：\n[root@node01 ~]# bash hello.sh Hello World! Shell 脚本执行 # 脚本的执行并非只有bash和sh，还有source和.且它们之间还存在一些细微的差异，接下来我们详细的给大家讲解一下。\n使用路径 # 格式：相对路径/脚本.sh或绝对路径/脚本.sh。\n注意：脚本文件必须为可执行文件（拥有 x 权限）。\n范例：\n[root@node01 ~]# ls -l total 4 -rw-r--r-- 1 root root 33 May 29 11:57 hello.sh [root@node01 ~]# /root/hello.sh bash: /root/hello.sh: Permission denied [root@node01 ~]# chmod ug+x hello.sh [root@node01 ~]# ./hello.sh Hello World! [root@node01 ~]# /root/hello.sh Hello World! bash 或 sh # 格式：bash 脚本.sh或sh 脚本.sh。\n范例：\n[root@node01 ~]# ls -l total 4 -rw-r--r-- 1 root root 33 May 29 11:57 hello.sh [root@node01 ~]# bash hello.sh Hello World! [root@node01 ~]# sh hello.sh Hello World! source 或 . # 格式：source 脚本.sh或. 脚本.sh\n范例：\n[root@node01 ~]# ls -l total 4 -rw-r--r-- 1 root root 33 May 29 11:57 hello.sh [root@node01 ~]# . hello.sh Hello World! [root@node01 ~]# source hello.sh Hello World! 区别 # bash 或 sh 执行脚本时会新开一个 bash，不同 bash 中的变量无法共享。而 source 或 . 是在同一个 bash 里面执行的，所以变量可以共享。\n范例：\n[root@node01 ~]# cat hello.sh #!/bin/bash echo \u0026#34;Hello World!\u0026#34; echo ${name} [root@node01 ~]# name=mrhelloworld [root@node01 ~]# bash hello.sh Hello World! [root@node01 ~]# sh hello.sh Hello World! [root@node01 ~]# . hello.sh Hello World! mrhelloworld [root@node01 ~]# source hello.sh Hello World! mrhelloworld 在脚本中添加ping baidu.com的指令，用不同的方式重新执行脚本并查看进程。\n[root@node01 ~]# echo \u0026#34;ping baidu.com\u0026#34; \u0026gt;\u0026gt; hello.sh [root@node01 ~]# cat hello.sh #!/bin/bash echo \u0026#34;Hello World!\u0026#34; echo ${name} ping baidu.com bash：\n[root@node01 ~]# bash hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4494 4447 0 12:17 pts/0 00:00:00 bash hello.sh root 4495 4494 0 12:17 pts/0 00:00:00 ping baidu.com sh：\n[root@node01 ~]# sh hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4497 4447 0 12:18 pts/0 00:00:00 sh hello.sh root 4498 4497 0 12:18 pts/0 00:00:00 ping baidu.com source：\n[root@node01 ~]# source hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4502 4447 0 12:19 pts/0 00:00:00 ping baidu.com .：\n[root@node01 ~]# . hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4508 4447 0 12:25 pts/0 00:00:00 ping baidu.com 怎么解决这个问题呢？可以使用export命令，它可以将当前进程的变量传递给子进程去使用，如下：\n[root@node01 ~]# export name=mrhelloworld [root@node01 ~]# bash hello.sh Hello World! mrhelloworld 所以，将来在配置环境变量（profile 文件）的时候，所有的变量前必须加export。\nShell 基础 # 注释 # 单行注释 # 要在 bash 中编写单行注释，必须在注释的开头使用井号#。\n#!/bin/bash # 我是注释 多行注释 # 有两种方法可以在 bash 脚本中插入多行注释：\n通过在\u0026lt;\u0026lt; COMMENT和COMMENT之间加上注释，可以在 bash 脚本中编写多行注释。 也可以通过将注释括在: '和单引号'之间来编写多行注释。 #!/bin/bash \u0026lt;\u0026lt; EOF 我是注释 我是注释 我是注释 EOF echo \u0026#34;Hello World!\u0026#34; 提示：EOF 表示 End Of File，表示文件结尾，这里代指从哪开始到哪结束。EOF 只是一个名称而已，可以使用任意非关键字名称进行替换，例如 COMMENT，通常都使用 EOF。\n或者：\n#!/bin/bash : \u0026#39; 我是注释 我是注释 我是注释 \u0026#39; echo \u0026#34;Hello World\u0026#34; 变量 # 语法 # 变量是将数据或有用的信息作为值存储的容器。变量的值可以更改，并且可以多次使用。变量是任何类型的数据（例如整数，浮点数，字符等）的临时存储。\n定义变量时，变量名不加$符号，而引用变量时则需要使用$。同其他编程语言一样，Shell 的变量声明也需要遵循一定的规则：\n可以包含字母，数字和下划线。 只能以字母和下划线开头，不能定义以任何数字开头的变量名称。 严格区分大小写。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 变量名称与值之间的等号=的两侧不能有空格。 不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 Shell 声明变量的语法格式如下：\nvariable=value variable=\u0026#39;value\u0026#39; variable=\u0026#34;value\u0026#34; variable 是变量名，value 是赋给变量的值。如果 value 不包含任何空白符（例如空格、Tab 缩进等），那么可以不使用引号；如果 value 包含了空白符，那么就必须使用引号包围起来。使用单引号和使用双引号也是有区别的，稍后我们会详细说明。\n变量定义举例：\n# 变量的声明与赋值 name=\u0026#34;zhangsan\u0026#34; # 变量的调用 echo $name echo ${name} # 修改变量的值，已定义的变量，可以被重新赋值 name=\u0026#34;lisi\u0026#34; # 只读变量 url=\u0026#34;https://www.baidu.com\u0026#34; readonly url # 测试只读变量是否可以被修改 url=\u0026#34;https://www.google.com\u0026#34; # 删除变量 unset name # 将命令结果复制给变量 info=`ls /usr/` info=$(ls /usr/) 调用变量时，变量名外面的花括号{}是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况：\nskill=\u0026#34;Shell\u0026#34; echo \u0026#34;I am good at ${skill}Script\u0026#34; 如果不给 skill 变量加花括号，写成echo \u0026quot;I am good at $skillScript\u0026quot;，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。\n调用变量时，推荐给所有变量加上花括号{}，这是个良好的编程习惯。\nShell 也支持将命令的执行结果赋值给变量，常见的有以下两种方式：\nvariable=`command` variable=$(command) 第一种方式把命令用反引号（位于 Esc 键的下方）包围起来，反引号和单引号非常相似，容易产生混淆，所以不推荐使用这种方式；第二种方式把命令用$()包围起来，区分更加明显，所以推荐使用这种方式。\n类型 # 局部变量：局部变量在脚本或命令中定义，仅在当前 Shell 实例中有效，其他 Shell 启动的程序不能访 问局部变量。例如，不同会话创建的变量无法互相访问。 环境变量：所有的程序，包括 Shell 启动的程序，都能访问环境变量，有些程序需要环境变量来保证 其正常运行。 Shell 变量：Shell 变量是由 Shell 程序设置的特殊变量。Shell 变量中有一部分是环境变量，有一部分是 局部变量。例如： # bash 在操作系统中具体的位置 echo ${BASH} # bash 版本信息 echo ${BASH_VERSION} # 操作系统的类型 echo $OSTYPE # 当前登录用户 echo ${USERNAME} # 当前用户家目录 echo ${HOME} # 当前的工作目录 echo ${PWD} 引号 # 当希望变量存储更复杂的值时，就需要使用引号。引号用于处理带有空格字符的文本和文件名。这是因为 Bash 使用空格来确定单独的项目。在 Shell 中，变量的值可以由单引号' '包围，也可以由双引号\u0026quot; \u0026quot;包围，它们到底有什么区别呢？不妨以下面的代码为例来说明：\n#!/bin/bash name=\u0026#34;zhangsan\u0026#34; echo \u0026#39;My name is ${name}\u0026#39; echo \u0026#34;My name is ${name}\u0026#34; 运行结果如下：\nMy name is ${name} My name is zhangsan 单引号' '包围变量的值时，单引号里面是什么就输出什么，即使内容中有变量和命令（命令需要反引起来）也会把它们原样输出。这种方式比较适合定义显示纯字符串的情况，即不希望解析变量、命令等的场景。\n双引号\u0026quot; \u0026quot;包围变量的值时，输出时会先解析里面的变量和命令，而不是把双引号中的变量名和命令原样输出。这种方式比较适合字符串中附带有变量和命令并且想将其解析后再输出的变量定义。\n位置参数 # 运行 Shell 脚本文件时我们还可以给它传递一些参数，这些参数在脚本文件内部可以使用$n的形式来接收。例如，$1表示第一个参数，$2表示第二个参数，依次类推。\n这种通过$n的形式来接收的参数，在 Shell 中称为位置参数。在讲解变量的命名时，我们提到变量的名字必须以字母或者下划线开头，不能以数字开头；但是位置参数却偏偏是数字，这和变量的命名规则是相悖的，所以我们将它们视为“特殊变量”。\n注意：如果参数个数太多，达到或者超过了 10 个，那么就得用${n}的形式来接收了，例如 ${10}、${23}。{}的作用是为了帮助解释器识别参数的边界，这跟使用变量时加{}是一样的效果。\n除了$n，Shell 中还有$#、$*、$@、$?、$$几个特殊参数，我们将在下节讲解。\n范例 test.sh：\n#!/bin/bash echo \u0026#34;name: $1\u0026#34; echo \u0026#34;age: $2\u0026#34; 运行 test.sh，并附带参数：\n[root@node01 ~]# bash test.sh zhangsan 18 name: zhangsan age: 18 特殊变量 # 上节我们讲到了 $n，它是特殊变量的一种，用来接收位置参数。本节我们继续讲解剩下的几个特殊变量，它们分别是：$#、$*、$@、$?、$$。\n变量 含义 $$ 当前 Shell 进程 ID。对于 Shell 脚本，就是这些脚本所在的进程 ID。 $0 当前脚本的文件名。 $n 传递给脚本或函数的参数。n 为数字，表示第几个参数。例如，第一个参数是 $1，第二个参数是 $2。 $@ 传递给脚本或函数的所有参数。 $* 传递给脚本或函数的所有参数。当被双引号\u0026quot; \u0026quot;包含时，$@ 与 $* 稍有不同。 $# 传递给脚本或函数的参数个数。 $? 上个命令的退出状态，或函数的返回值。 $*和$@作用都是获取传递给脚本或函数的所有参数。在没有被双引号包围时，两者没有区别，接收到的每个参数都是独立的，用空格分隔。\n当被双引号包围时，$@与没有被双引号包围时没有变化，每个参数依然是独立的。但是$*被双引号包围时，会将所有参数看作一个整体。\n$?是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。所谓退出状态，就是上一个命令执行后的返回结果。退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1，这和 C 语言的 main() 函数是类似的。\n范例 test.sh：\n#!/bin/bash echo \u0026#34;Process ID: $$\u0026#34; echo \u0026#34;File Name: $0\u0026#34; echo \u0026#34;First Parameter: $1\u0026#34; echo \u0026#34;Second Parameter: $2\u0026#34; echo \u0026#34;All parameters 1: $@\u0026#34; echo \u0026#34;All parameters 2: $*\u0026#34; echo \u0026#34;Total: $#\u0026#34; echo \u0026#34;--------------------\u0026#34; for var in \u0026#34;$@\u0026#34; do echo ${var} done echo \u0026#34;--------------------\u0026#34; for var in \u0026#34;$*\u0026#34; do echo ${var} done 运行 test.sh，并附带参数：\n[root@node01 ~]# bash test.sh Linux Shell Process ID: 4779 File Name: test.sh First Parameter: Linux Second Parameter: Shell All parameters 1: Linux Shell All parameters 2: Linux Shell Total: 2 -------------------- Linux Shell -------------------- Linux Shell $?是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。所谓退出状态，就是上一个命令执行后的返回结果。退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1，这和C语言的 main() 函数是类似的。不过，也有一些命令返回其他值，表示不同类型的错误。\n范例1，$? 获取上一个命令的退出状态。test.sh：\n#!/bin/bash if [ \u0026#34;$1\u0026#34; == 100 ] then exit 0 # 参数正确，退出状态为 0 else exit 1 # 参数错误，退出状态 1 fi 例如，运行 test.sh 时传递参数 100：\n[root@node01 ~]# bash test.sh 100 [root@node01 ~]# echo $? 0 再如，运行 test.sh 时传递参数 50：\n[root@node01 ~]# bash test.sh 50 [root@node01 ~]# echo $? 1 范例2，$? 获取函数的返回值。test.sh：\n#!/bin/bash # 得到两个数相加的和 function add() { return `expr $1 + $2` } # 调用函数 add 20 30 echo $? # 获取函数返回值 运行结果如下：\n[root@node01 ~]# bash test.sh 50 有 C++、C#、Java 等编程经验的读者请注意：严格来说，Shell 函数中的 return 关键字用来表示函数的退出状态，而不是函数的返回值；Shell 不像其它编程语言，没有专门处理返回值的关键字。\n范例 2 在其它编程语言中没有任何问题，但是在 Shell 中是非常错误的，Shell 函数的返回值和其它编程语言大有不同，我们将在后面的 Shell 函数返回值中展开讨论。\n字符串 # 定义 # 字符串（String）就是一系列字符的组合。字符串是 Shell 编程中最常用的数据类型之一（除了数字和字符串，也没有其他类型了）。\n字符串可以由单引号' '包围，也可以由双引号\u0026quot; \u0026quot;包围，也可以不用引号。\n由单引号' '包围的字符串：\n任何字符都会原样输出，在其中使用变量是无效的。\n字符串中不能出现单引号，即使对单引号进行转义（\\'）也不行。\n由双引号\u0026quot; \u0026quot;包围的字符串：\n如果其中包含了某个变量，那么该变量会被解析（得到该变量的值），而不是原样输出。\n字符串中可以出现双引号，只要它被转义（\\\u0026quot;）就行。\n不被引号包围的字符串：\n不被引号包围的字符串中出现变量时也会被解析，这一点和双引号\u0026quot; \u0026quot;包围的字符串一样。\n字符串中不能出现空格，否则空格后边的字符串会作为其他变量或者命令解析。\n长度 # 在 Shell 中获取字符串长度很简单，具体方法如下：\n[root@node01 ~]# name=mrhelloworld [root@node01 ~]# echo ${#name} 12 拼接 # 在脚本语言中，字符串的拼接（也称字符串连接或者字符串合并）往往都非常简单，例如：\n在 PHP 中，使用 . 即可连接两个字符串； 在 JavaScript 中，使用 + 即可将两个字符串合并为一个。 然而，在 Shell 中你不需要使用任何运算符，只需要将两个字符串并排放在一起就能实现拼接，非常简单粗暴。范例 test.sh：\n#!/bin/bash name=\u0026#34;zhangsan\u0026#34; age=18 str1=$name$age # 中间不能有空格 str2=\u0026#34;$name $age\u0026#34; # 如果被双引号包围，那么中间可以有空格 str3=$name\u0026#34;: \u0026#34;$age # 中间可以出现别的字符串 str4=\u0026#34;$name: $age\u0026#34; # 这样写也可以 str5=\u0026#34;${name}同学: ${age}岁\u0026#34; # 这个时候需要给变量名加上大括号 echo $str1 echo $str2 echo $str3 echo $str4 echo $str5 运行 test.sh：\n[root@node01 ~]# bash test.sh zhangsan18 zhangsan 18 zhangsan: 18 zhangsan: 18 zhangsan同学: 18岁 截取 # Shell 截取字符串通常有两种方式：\n从指定位置开始截取 从指定字符（子字符串）开始截取 从指定位置开始截取 # 这种方式需要两个参数：除了指定起始位置，还需要截取长度，才能最终确定要截取的字符串。\n既然需要指定起始位置，那么就涉及到计数方向的问题，到底是从字符串左边开始计数，还是从字符串右边开始计数。答案是 Shell 同时支持两种计数方式。\n如果想从字符串的左边开始计数，那么截取字符串的具体格式如下：\n${string:start:length} 其中，string 是要截取的字符串，start 是起始位置（从左边开始，从 0 开始计数），length 是要截取的长度（省略的话表示直到字符串的末尾）。\n范例：\n[root@node01 ~]# name=mrhelloworld [root@node01 ~]# echo ${name:2} helloworld [root@node01 ~]# echo ${name:2:5} hello 如果想从字符串的右边开始计数，那么截取字符串的具体格式如下：\n${string:0-start:length} 相比从左边开始计数仅仅多了0-，这是固定的写法，专门用来表示从字符串右边开始计数。\n范例：\n[root@node01 ~]# name=mrhelloworld [root@node01 ~]# echo ${name:0-5} world [root@node01 ~]# echo ${name:0-10} helloworld [root@node01 ~]# echo ${name:0-10:5} hello 从指定字符（子字符串）开始截取 # 这种截取方式无法指定字符串长度，只能从指定字符（子字符串）截取到字符串末尾。Shell 可以截取指定字符（子字符串）右边的所有字符，也可以截取左边的所有字符。\n使用#号可以截取指定字符（或者子字符串）右边的所有字符，具体格式如下：\n${string#*chars} 其中，string 表示要截取的字符串，chars 是指定的字符（或者子字符串），*是通配符的一种，表示任意长度的字符串。*chars连起来使用的意思是：忽略左边的所有字符，直到遇见 chars（chars 不会被截取）。\n范例：\n[root@node01 ~]# url=https://www.yjxxt.com [root@node01 ~]# echo ${url#*https://} www.yjxxt.com [root@node01 ~]# echo ${url#*://} www.yjxxt.com 注意，以上写法遇到第一个匹配的字符（子字符串）就结束了，如果希望直到最后一个指定字符（子字符串）再匹配结束，那么可以使用##。例如：\n[root@node01 ~]# echo ${url#*.} yjxxt.com [root@node01 ~]# echo ${url##*.} com 使用%号可以截取指定字符（或者子字符串）左边的所有字符，具体格式如下：\n${string%chars*} 请注意*的位置，因为要截取 chars 左边的字符，而忽略 chars 右边的字符，所以*应该位于 chars 的右侧。其他方面%和#的用法相同，这里不再赘述，仅举例说明：\n[root@node01 ~]# url=https://www.yjxxt.com [root@node01 ~]# echo ${url%www*} https:// [root@node01 ~]# echo ${url%.*} https://www.yjxxt [root@node01 ~]# echo ${url%%.*} https://www 总结 # 格式 说明 ${string:start:length} 从 string 字符串的左边第 start 个字符开始，向右截取 length 个字符。 ${string:start} 从 string 字符串的左边第 start 个字符开始截取，向右直到最后。 ${string:0-start:length} 从 string 字符串的右边第 start 个字符开始，向左截取 length 个字符。 ${string:0-start} 从 string 字符串的右边第 start 个字符开始截取，向左直到最后。 ${string#*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string##*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string%*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 ${string%%*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 数组 # 和其他编程语言一样，Shell 也支持数组。数组（Array）是若干数据的集合，其中的每一份数据都称为元素（Element）。\nShell 没有限制数组的大小，理论上可以存放无限量的数据。和 C++、Java 等类似，Shell 数组元素的下标也是从 0 开始计数。\n获取数组中的元素要使用下标[index]，下标可以是一个整数，也可以是一个结果为整数的表达式；当然，下标必须大于等于 0。遗憾的是，常用的 Bash Shell 只支持一维数组，不支持多维数组。\n定义 # 在 Shell 中，用括号()来表示数组，数组元素之间用空格来分隔。由此，定义数组的一般形式为：\narray_name=(ele1 ele2 ele3 ... elen) Shell 是弱类型的，它并不要求所有数组元素的类型必须相同，例如：\narr=(20 56 \u0026#34;mrhelloworld\u0026#34;) Shell 数组的长度不是固定的，定义之后还可以增加元素。例如，对于上面的 arr 数组，它的长度是 3，使用下面的代码会在最后增加一个元素，使其长度扩展到 4：\nnums[3]=88 此外，Shell 还支持只给特定元素赋值：\nages=([3]=24 [5]=19 [10]=12) 以上代码就只给第 3、5、10 个元素赋值，所以数组长度是 3。\n获取 # 获取数组元素的值，一般使用下面的格式：\n${array_name[index]} 其中，array_name 是数组名，index 是下标。例如：\n[root@node01 ~]# echo ${arr[2]} mrhelloworld 使用@或*可以获取数组中的所有元素，例如：\n[root@node01 ~]# echo ${arr[*]} 20 56 mrhelloworld [root@node01 ~]# echo ${ages[@]} 24 19 12 长度 # 所谓数组长度，就是数组元素的个数。使用@或*可以获取数组中的所有元素，然后使用#来获取数组元素的个数，所以获取数组长度的语法如下：\n[root@node01 ~]# echo ${#arr[@]} 3 [root@node01 ~]# echo ${#ages[*]} 3 如果某个元素是字符串，还可以通过指定下标的方式获得该元素的长度，如下所示：\n[root@node01 ~]# echo ${#arr[2]} 12 拼接 # 所谓 Shell 数组拼接（数组合并），就是将两个数组连接成一个数组。\n拼接数组的思路是：先利用@或*，将数组展开成列表，然后再合并到一起。具体格式如下：\narray_new=(${array1[@]} ${array2[@]}) array_new=(${array1[*]} ${array2[*]}) 两种方式是等价的，选择其一即可。其中，array1 和 array2 是需要拼接的数组，array_new 是拼接后形成的新数组。\n范例 test.sh：\n#!/bin/bash array1=(23 56) array2=(99 \u0026#34;mrhelloworld\u0026#34;) array_new=(${array1[@]} ${array2[*]}) echo ${array_new[@]} # 也可以写作 ${array_new[*]} 运行结果如下：\n[root@node01 ~]# bash test.sh 23 56 99 mrhelloworld 删除 # 在 Shell 中，使用 unset 关键字来删除数组元素，具体格式如下：\nunset array_name[index] 其中，array_name 表示数组名，index 表示数组下标。\n如果不写下标只写数组名，则表示删除整个数组，所有元素都会消失。\n范例 test.sh：\n#!/bin/bash arr=(23 56 99 \u0026#34;mrhelloworld\u0026#34;) unset arr[1] echo ${arr[@]} unset arr echo ${arr[*]} echo \u0026#39;--------------------\u0026#39; 运行结果如下：\n[root@node01 ~]# bash test.sh 23 99 mrhelloworld -------------------- Shell 高级 # Shell 运算符 # Shell 和其他编程语言一样，支持多种运算符，包括：\n算数运算符 关系运算符 逻辑运算符 字符串运算符 文件测试运算符 算数运算符 # 但是，Shell 和其它编程语言不同，Shell 不能直接进行算数运算，必须使用数学计算命令，这让初学者感觉很困惑，也让有经验的程序员感觉很奇葩。expr 是一款表达式计算工具，使用它能完成表达式的求值操作。\n#!/bin/bash a=10 b=20 val=$(expr $a + $b) echo \u0026#34;a + b : $val\u0026#34; val=`expr $a - $b` echo \u0026#34;a - b : $val\u0026#34; val=`expr $a \\* $b` echo \u0026#34;a * b : $val\u0026#34; val=`expr $b / $a` echo \u0026#34;b / a : $val\u0026#34; val=`expr $b % $a` echo \u0026#34;b % a : $val\u0026#34; if [ $a == $b ] then echo \u0026#34;a 等于 b\u0026#34; fi if [ $a != $b ] then echo \u0026#34;a 不等于 b\u0026#34; fi 关系运算符 # 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n#!/bin/bash a=10 b=20 if [ $a -eq $b ] then echo \u0026#34;$a -eq $b : a 等于 b\u0026#34; else echo \u0026#34;$a -eq $b: a 不等于 b\u0026#34; fi if [ $a -ne $b ] then echo \u0026#34;$a -ne $b: a 不等于 b\u0026#34; else echo \u0026#34;$a -ne $b : a 等于 b\u0026#34; fi if [ $a -gt $b ] then echo \u0026#34;$a -gt $b: a 大于 b\u0026#34; else echo \u0026#34;$a -gt $b: a 不大于 b\u0026#34; fi if [ $a -lt $b ] then echo \u0026#34;$a -lt $b: a 小于 b\u0026#34; else echo \u0026#34;$a -lt $b: a 不小于 b\u0026#34; fi if [ $a -ge $b ] then echo \u0026#34;$a -ge $b: a 大于或等于 b\u0026#34; else echo \u0026#34;$a -ge $b: a 小于 b\u0026#34; fi if [ $a -le $b ] then echo \u0026#34;$a -le $b: a 小于或等于 b\u0026#34; else echo \u0026#34;$a -le $b: a 大于 b\u0026#34; fi 逻辑运算符 # #!/bin/bash a=10 b=20 if [ $a != $b ] then echo \u0026#34;$a != $b : a 不等于 b\u0026#34; else echo \u0026#34;$a == $b: a 等于 b\u0026#34; fi if [ $a -lt 100 -a $b -gt 15 ] then echo \u0026#34;$a 小于 100 且 $b 大于 15 : 返回 true\u0026#34; else echo \u0026#34;$a 小于 100 且 $b 大于 15 : 返回 false\u0026#34; fi if [ $a -lt 100 -o $b -gt 100 ] then echo \u0026#34;$a 小于 100 或 $b 大于 100 : 返回 true\u0026#34; else echo \u0026#34;$a 小于 100 或 $b 大于 100 : 返回 false\u0026#34; fi if [ $a -lt 5 -o $b -gt 100 ] then echo \u0026#34;$a 小于 5 或 $b 大于 100 : 返回 true\u0026#34; else echo \u0026#34;$a 小于 5 或 $b 大于 100 : 返回 false\u0026#34; fi #!/bin/bash a=10 b=20 if [[ $a -lt 100 \u0026amp;\u0026amp; $b -gt 100 ]] then echo \u0026#34;返回 true\u0026#34; else echo \u0026#34;返回 false\u0026#34; fi if [[ $a -lt 100 || $b -gt 100 ]] then echo \u0026#34;返回 true\u0026#34; else echo \u0026#34;返回 false\u0026#34; fi 字符串运算符 # #!/bin/bash a=\u0026#34;abc\u0026#34; b=\u0026#34;efg\u0026#34; if [ $a = $b ] then echo \u0026#34;$a = $b : a 等于 b\u0026#34; else echo \u0026#34;$a = $b: a 不等于 b\u0026#34; fi if [ $a != $b ] then echo \u0026#34;$a != $b : a 不等于 b\u0026#34; else echo \u0026#34;$a != $b: a 等于 b\u0026#34; fi if [ -z $a ] then echo \u0026#34;-z $a : 字符串长度为 0\u0026#34; else echo \u0026#34;-z $a : 字符串长度不为 0\u0026#34; fi if [ -n \u0026#34;$a\u0026#34; ] then echo \u0026#34;-n $a : 字符串长度不为 0\u0026#34; else echo \u0026#34;-n $a : 字符串长度为 0\u0026#34; fi if [ $a ] then echo \u0026#34;$a : 字符串不为空\u0026#34; else echo \u0026#34;$a : 字符串为空\u0026#34; fi 文件测试运算符 # 操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ] 返回 false。 -p file 检测文件是否是有名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 #!/bin/bash file=\u0026#34;/root/test.sh\u0026#34; if [ -r $file ] then echo \u0026#34;文件可读\u0026#34; else echo \u0026#34;文件不可读\u0026#34; fi if [ -w $file ] then echo \u0026#34;文件可写\u0026#34; else echo \u0026#34;文件不可写\u0026#34; fi if [ -x $file ] then echo \u0026#34;文件可执行\u0026#34; else echo \u0026#34;文件不可执行\u0026#34; fi if [ -f $file ] then echo \u0026#34;文件为普通文件\u0026#34; else echo \u0026#34;文件为特殊文件\u0026#34; fi if [ -d $file ] then echo \u0026#34;文件是个目录\u0026#34; else echo \u0026#34;文件不是个目录\u0026#34; fi if [ -s $file ] then echo \u0026#34;文件不为空\u0026#34; else echo \u0026#34;文件为空\u0026#34; fi if [ -e $file ] then echo \u0026#34;文件存在\u0026#34; else echo \u0026#34;文件不存在\u0026#34; fi echo 打印数据 # ## 显示普通字符串 echo \u0026#34;Hello World\u0026#34; ## 显示转义字符 echo \u0026#34;\\\u0026#34;Hello World\\\u0026#34;\u0026#34; ## 显示变量 name=\u0026#34;zhangsan\u0026#34; echo \u0026#34;$name Hello World\u0026#34; ## 显示换行 echo -e \u0026#34;OK! \\n\u0026#34; echo \u0026#34;Hello World\u0026#34; ## 显示不换行 echo -e \u0026#34;OK! \\c\u0026#34; echo \u0026#34;Hello World\u0026#34; ## 显示结果定向至文件 echo \u0026#34;Hello World\u0026#34; \u0026gt; myfile ## 原样输出字符串 echo \u0026#39;$name\\\u0026#34;\u0026#39; ## 显示命令执行结果，推荐方式 echo $(date) ## 显示命令执行结果 echo `date` test 命令 # Shell 中的 test 命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试。\n数字\n字符串\n文件测试\n# 比较 num1=100 num2=100 if test $[num1] -eq $[num2] then echo \u0026#39;两个数相等！\u0026#39; else echo \u0026#39;两个数不相等！\u0026#39; fi Shell 流程控制 # if # if condition1 then command1 elif condition2 then command2 else commandN fi a=10 b=20 if [ $a == $b ] then echo \u0026#34;a 等于 b\u0026#34; elif [ $a -gt $b ] then echo \u0026#34;a 大于 b\u0026#34; elif [ $a -lt $b ] then echo \u0026#34;a 小于 b\u0026#34; else echo \u0026#34;没有符合的条件\u0026#34; fi case # case 语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。 case 值 in 模式1) command1 command2 ... commandN ;; 模式2) command1 command2 ... commandN ;; esac echo \u0026#39;输入 1 到 4 之间的数字:\u0026#39; echo \u0026#39;你输入的数字为:\u0026#39; read num case $num in 1) echo \u0026#39;你选择了 1\u0026#39; ;; 2) echo \u0026#39;你选择了 2\u0026#39; ;; 3) echo \u0026#39;你选择了 3\u0026#39; ;; 4) echo \u0026#39;你选择了 4\u0026#39; ;; *) echo \u0026#39;你没有输入 1 到 4 之间的数字\u0026#39; ;; esac for # 当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。\n命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。\nin列表是可选的，如果不用它，for循环使用命令行的位置参数。\nfor var in item1 item2 ... itemN do command1 command2 ... commandN done for loop in 1 2 3 4 5 do echo \u0026#34;The value is: $loop\u0026#34; done for str in \u0026#39;This is a string\u0026#39;,\u0026#39;hello moto\u0026#39; do echo $str done while循环 # while 循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。 while condition do command done # Bash let 命令，它用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量 #!/bin/bash i=1 while(( $i\u0026lt;=5 )) do echo $i let \u0026#34;i++\u0026#34; done # 无限循环 while true do command done break # break 命令允许跳出所有循环（终止执行后面的所有循环）。 #!/bin/bash while : do echo -n \u0026#34;输入 1 到 5 之间的数字:\u0026#34; read aNum case $aNum in 1|2|3|4|5) echo \u0026#34;你输入的数字为 $aNum!\u0026#34; ;; *) echo \u0026#34;你输入的数字不是 1 到 5 之间的! 游戏结束\u0026#34; break ;; esac done continue # continue 命令不会跳出所有循环，仅仅跳出当前循环。 #!/bin/bash while : do echo -n \u0026#34;输入 1 到 5 之间的数字: \u0026#34; read aNum case $aNum in 1|2|3|4|5) echo \u0026#34;你输入的数字为 $aNum!\u0026#34; ;; *) echo \u0026#34;你输入的数字不是 1 到 5 之间的!\u0026#34; continue echo \u0026#34;游戏结束\u0026#34; ;; esac done Shell 函数 # linux shell 可以用户定义函数，然后在shell脚本中可以随便调用。 可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。 参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 #!/bin/bash ## 第一个函数------------------------------ demoFun(){ echo \u0026#34;这是我的第一个 shell 函数!\u0026#34; } echo \u0026#34;-----函数开始执行-----\u0026#34; demoFun echo \u0026#34;-----函数执行完毕-----\u0026#34; ## 函数返回值------------------------------ funWithReturn(){ echo \u0026#34;这个函数会对输入的两个数字进行相加运算...\u0026#34; echo \u0026#34;输入第一个数字: \u0026#34; read aNum echo \u0026#34;输入第二个数字: \u0026#34; read anotherNum echo \u0026#34;两个数字分别为 $aNum 和 $anotherNum !\u0026#34; return $(($aNum+$anotherNum)) } funWithReturn # 函数返回值在调用该函数后通过 $? 来获得。 echo \u0026#34;输入的两个数字之和为 $? !\u0026#34; ## 函数参数------------------------------ funWithParam(){ echo \u0026#34;第一个参数为 $1 !\u0026#34; echo \u0026#34;第二个参数为 $2 !\u0026#34; echo \u0026#34;第十个参数为 $10 !\u0026#34; # 如果参数个数太多，达到或者超过了 10 个，那么就得用 ${n} 的形式来接收了 echo \u0026#34;第十个参数为 ${10} !\u0026#34; echo \u0026#34;第十一个参数为 ${11} !\u0026#34; echo \u0026#34;参数总数有 $# 个!\u0026#34; echo \u0026#34;作为一个字符串输出所有参数 $* !\u0026#34; } funWithParam 1 2 3 4 5 6 7 8 9 Shell 实战 # 添加开机启动项 # 需求：服务器开机后自动与 cn.ntp.org.cn 同步时间。\ntouch /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;yum info ntp \u0026amp;\u0026amp; yum -y install ntp \u0026amp;\u0026amp; ntpdate cn.ntp.org.cn\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh chmod +x /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;/usr/local/scripts/auto_ntpdate.sh\u0026#39; \u0026gt;\u0026gt; /etc/rc.d/rc.local chmod +x /etc/rc.d/rc.local 虚拟机初始化脚本 # 目标服务器环境如下：\n主机名 IP node01 192.168.88.101 node02 192.168.88.102 node03 192.168.88.103 首先，使用最初始的example虚拟机克隆出一台完整虚拟机。然后，启动虚拟机并修改网络配置与主机名：\n修改网络配置中的IPADDR并重启网络； echo node01 \u0026gt; /etc/hostname修改主机名。 接下来，sh init.sh运行脚本。\n最后，拍摄快照方便后期回退。\n然后通过已经初始化完成的 node01 完整克隆出 node02 和 node03，修改它两的网络配置与主机名即可。\n虚拟机初始化脚本init.sh完整内容如下：\n#!/bin/bash ## -bash: ./init.sh: /bin/bash^M: bad interpreter: No such file or directory ## vim 或者 vi 的命令模式下，输入命令 set fileformat=unix 即可解决上述问题 echo -e \u0026#34;\\e[1;44m【在 /opt 目录和 /var 目录下创建 yjx 目录，在 /usr/local 目录下创建 scripts 目录】\\e[0m\u0026#34; sleep 2 mkdir -p /opt/yjx /var/yjx /usr/local/scripts echo -e \u0026#34;\\e[1;44m【关闭并禁用 firewalld 防火墙】\\e[0m\u0026#34; sleep 2 systemctl stop firewalld systemctl disable firewalld systemctl status firewalld echo -e \u0026#34;\\e[1;44m【关闭 SELinux】\\e[0m\u0026#34; sleep 2 sed -i \u0026#39;/^SELINUX=/c SELINUX=disabled\u0026#39; /etc/selinux/config echo -e \u0026#34;\\e[1;44m【安装 wget】\\e[0m\u0026#34; sleep 2 yum -y install wget echo -e \u0026#34;\\e[1;44m【修改 yum 源为阿里源】\\e[0m\u0026#34; sleep 2 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum clean all yum makecache echo -e \u0026#34;\\e[1;44m【安装常用依赖】\\e[0m\u0026#34; sleep 2 yum -y install man man-pages telnet perl net-tools openssl-devel ntp lrzsz zip unzip tree vim rsync echo -e \u0026#34;\\e[1;44m【修改时区为 Asia/Shanghai】\\e[0m\u0026#34; sleep 2 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo -e \u0026#34;\\e[1;44m【与中国 NTP 时间服务器 cn.ntp.org.cn 进行时间同步】\\e[0m\u0026#34; sleep 2 yum info ntp \u0026amp;\u0026amp; yum -y install ntp \u0026amp;\u0026amp; ntpdate cn.ntp.org.cn echo -e \u0026#34;\\e[1;44m【修改 hosts 文件，添加集群环境机器 IP 与域名映射】\\e[0m\u0026#34; sleep 2 echo \u0026#34;192.168.88.100 basenode\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.88.101 node01\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.88.102 node02\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.88.103 node03\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo -e \u0026#34;\\e[1;44m【安装 JDK 并设置环境变量】\\e[0m\u0026#34; sleep 2 rpm -ivh jdk-8u351-linux-x64.rpm echo \u0026#39;export JAVA_HOME=/usr/java/jdk1.8.0_351-amd64\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;export PATH=$JAVA_HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; /etc/profile source /etc/profile echo -e \u0026#34;\\e[1;44m【安装 Tomcat】\\e[0m\u0026#34; sleep 2 tar -zxf apache-tomcat-9.0.72.tar.gz -C /opt/yjx/ echo -e \u0026#34;\\e[1;44m【安装 MySQL】\\e[0m\u0026#34; sleep 2 rpm -e --nodeps `rpm -qa | grep mariadb` tar -xvf mysql-8.0.18-1.el7.x86_64.rpm-bundle.tar rpm -ivh mysql-community-common-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-client-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-server-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-devel-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-compat-8.0.18-1.el7.x86_64.rpm systemctl start mysqld systemctl enable mysqld temppasswd=`grep \u0026#34;A temporary password\u0026#34; /var/log/mysqld.log | awk \u0026#39;{print $NF}\u0026#39;` mysql -uroot -p$temppasswd --connect-expired-password \u0026lt;\u0026lt; EOF SET GLOBAL validate_password.policy = low; SET GLOBAL validate_password.length = 6; ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; USE mysql; UPDATE user SET host = \u0026#39;%\u0026#39; WHERE user = \u0026#39;root\u0026#39;; COMMIT; FLUSH PRIVILEGES; EXIT EOF systemctl restart mysqld echo -e \u0026#34;\\e[1;44m【添加时间同步服务至开机启动】\\e[0m\u0026#34; sleep 2 touch /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;yum info ntp \u0026amp;\u0026amp; yum -y install ntp \u0026amp;\u0026amp; ntpdate cn.ntp.org.cn\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh chmod +x /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;/usr/local/scripts/auto_ntpdate.sh\u0026#39; \u0026gt;\u0026gt; /etc/rc.d/rc.local chmod +x /etc/rc.d/rc.local echo -e \u0026#34;\\e[1;44m【删除 JDK Tomcat MySQL 安装包和虚拟机初始化脚本】\\e[0m\u0026#34; sleep 2 rm jdk* -rf rm apache-tomcat* -rf rm mysql* -rf rm init.sh -rf echo -e \u0026#34;\\e[1;41m【即将关闭计算机】\\e[0m\u0026#34; sleep 2 shutdown -h now 服务器相互免秘钥 # 生成密钥 # 分别在三台机器上运行以下命令生成密钥对：\nssh-keygen -t rsa -P \u0026#39;\u0026#39; -f ~/.ssh/id_rsa 运行以上命令后会在~/.ssh/目录下生成一对密钥对。\n[root@node01 ~]# ls ~/.ssh/ id_rsa id_rsa.pub known_hosts 取消主机名与 host 校验 # 分别在三台机器上修改/etc/ssh/ssh_config文件的配置，在Host *节点下配置以下信息：\n# 严格的密钥检查 no StrictHostKeyChecking no # 如果不希望生成已知主机列表文件，可以将已知主机列表文件信息写入黑洞（不会再生成 known_hosts 文件） #UserKnownHostsFile /dev/null 这样以后再也不会弹出将该主机添加到当前设备的已知主机列表中的提示信息了。\n如果将已知主机列表文件信息写入了黑洞，那么远程访问时会弹出以下警告：\nWarning: Permanently added \u0026#39;node02,192.168.88.102\u0026#39; (ECDSA) to the list of known hosts. 这个警告不影响任何操作，只是看着比较碍眼。解决办法：在文件夹~/.ssh/下创建config文件，命令如下：\nvim ~/.ssh/config 在新建的文件中写入如下内容：LogLevel=quiet。\n拷贝公钥 # 接下来把自己的公钥互相传递给其他主机，这个公钥文件必须放在对方主机的~/.ssh/authorized_keys文件中。可以使用命令将公钥文件自动传递过去，分别在三台机器运行以下命令：\nssh-copy-id -i ~/.ssh/id_rsa.pub root@node01 ssh-copy-id -i ~/.ssh/id_rsa.pub root@node02 ssh-copy-id -i ~/.ssh/id_rsa.pub root@node03 前面已经通过脚本修改了 hosts 文件，添加了集群环境机器 IP 与域名映射，所以这里可以直接使用主机名。\n传输文件测试是否已免密或者使用 ssh 协议登录对方主机进行测试：\n[root@localhost ~]# scp anaconda-ks.cfg root@node02:~ Warning: Permanently added \u0026#39;node02,192.168.88.102\u0026#39; (ECDSA) to the list of known hosts. anaconda-ks.cfg [root@localhost ~]# ssh root@node02 Warning: Permanently added \u0026#39;node02,192.168.88.102\u0026#39; (ECDSA) to the list of known hosts. Last login: Sat Jun 4 21:07:25 2022 from node01 测试没问题后，shutdown -h now关机，拍摄快照方便后期回退。\n集群启动脚本 # 在/usr/local/bin目录下创建对应服务的脚本：\n[root@node01 ~]# vim /usr/local/bin/tomcat tomcat脚本内容如下：\n#!/bin/bash user=$(whoami) case $1 in \u0026#34;start\u0026#34;) for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i Tomcat 启动 ====================\\e[0m\u0026#34; ssh $user@$i \u0026#34;/opt/yjx/apache-tomcat-9.0.72/bin/startup.sh\u0026#34; done ;; \u0026#34;shutdown\u0026#34;) for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i Tomcat 停止 ====================\\e[0m\u0026#34; ssh $user@$i \u0026#34;/opt/yjx/apache-tomcat-9.0.72/bin/shutdown.sh\u0026#34; done ;; esac 修改脚本权限为用户读写执行rwx，组读执行r-x，其他用户无权限r--：\n[root@node01 ~]# chmod 754 /usr/local/bin/tomcat JPS 脚本 # jps 是 JDK 提供的一个查看当前系统 Java 进程的小工具，全称是 Java Virtual Machine Process Status Tool。\n常用选项如下：\n-q：忽略输出的类名，Jar 名以及传递给 main 方法的参数，只输出 PID -m：输出传递给 main 方法的参数，如果是内嵌的 JVM 则输出为 null -l：输出应用程序主类的完整包名，或者是应用程序 JAR 文件的完整路径 -v：输出 JVM 的参数 -V：输出通过标记文件传递给 JVM 的参数(.hotspotrc 文件，或者通过参数-XX:Flags=\u0026lt;filename\u0026gt;指定的文件) -J：传递 JVM 参数到由 javac 调用的 java 加载器中，例如：-J-Xms512m，把启动内存设置为 512M。使用 -J 选项可以非常方便的向基于 Java 开发的底层虚拟机应用程序传递参数 顺便再创建一个查看所有服务器 JPS 进程的脚本。\n[root@node01 ~]# vim /usr/local/bin/jpsall jpsall脚本内容如下：\n#!/bin/bash # 获取当前用户名称 user=$(whoami) # $#：传递给脚本或函数的参数个数 params_count=$# # 如果没有参数，直接运行 \u0026#34;jps\u0026#34; if [ $params_count -lt 1 ] then for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i ====================\\e[0m\u0026#34; ssh $user@$i jps done exit fi # 如果有参数，运行 \u0026#34;jps -参数\u0026#34; for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i ====================\\e[0m\u0026#34; params=\u0026#34;\u0026#34; for p in $@ do params+=\u0026#34;$p \u0026#34; done ssh $user@$i \u0026#34;jps $params\u0026#34; done 修改脚本权限为用户读写执行rwx，组读执行r-x，其他用户读执行r-x：\n[root@node01 ~]# chmod 755 /usr/local/bin/jpsall 文件分发脚本 # 在/usr/local/bin目录下创建yjxrsync脚本，如下：\n[root@node01 ~]# vim /usr/local/bin/yjxrsync yjxrsync脚本内容如下：\n#!/bin/bash # 获取输入参数的个数 param_count=$# # 如果没有参数，直接退出 if [ $param_count -lt 1 ] then echo -e \u0026#34;\\e[1;31myjxrsync: You must pass in the file name parameter.\\e[0m\u0026#34; exit fi # 获取当前用户名称 user=$(whoami) # 如果有参数，遍历参数（文件或目录名称） for p in $@ do echo -e \u0026#34;\\e[1;34m==================== $p 开始同步 ====================\\e[0m\u0026#34; # basename：显示文件路径名的基本文件名，例如 /opt/bd 会显示 bd；/opt/bd/test.txt 会显示 test.txt file_name=$(basename $p) echo file_name=$file_name # 获取文件的上级目录的绝对路径 # -P：如果切换的目标目录是一个符号链接，则直接切换到符号链接指向的目标目录 parent_dir=`cd -P $(dirname $p); pwd` echo parent_dir=$parent_dir # 循环处理文件 for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i ====================\\e[0m\u0026#34; rsync -av --delete $parent_dir/$file_name $user@$i:$parent_dir done done 修改脚本权限为用户读写执行rwx，组读执行r-x，其他用户无权限---：\n[root@node01 ~]# chmod 750 /usr/local/bin/yjxrsync ","date":"2024-07-01","externalUrl":null,"permalink":"/docs/shell/","section":"Docs","summary":"Shell 编程概述 # Shell 本身并不是内核的一部分，它只是站在内核的基础上编写的一个应用程序，它和 QQ、迅雷、Firefox 等其它软件没有什么区别。然而 Shell 也有着它的特殊性，就是开机立马启动，并呈现在用户面前；用户通过 Shell 来使用 Linux，不启动 Shell 的话，用户就没办法使用 Linux。","title":"Shell","type":"docs"},{"content":"hhhh 使用PicGo图床了\n","date":"2024-07-01","externalUrl":null,"permalink":"/docs/test/","section":"Docs","summary":"hhhh 使用PicGo图床了","title":"test","type":"docs"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/hbase/","section":"Tags","summary":"","title":"HBase","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/picgo/","section":"Tags","summary":"","title":"PicGo","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/shell/","section":"Tags","summary":"","title":"Shell","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/","section":"Tags","summary":"","title":"大数据","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E5%88%86%E4%BA%AB/","section":"Tags","summary":"","title":"分享","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/","section":"河爷","summary":"","title":"河爷","type":"page"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/","section":"Tags","summary":"","title":"面试题","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E5%9B%BE%E5%BA%8A/","section":"Tags","summary":"","title":"图床","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E8%BF%90%E7%BB%B4/","section":"Tags","summary":"","title":"运维","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]